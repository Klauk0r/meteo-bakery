{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meteo Bakery - Baseline Model\n",
    "As our first baseline model, we will use a simple heuristic where we use the product sales from individual days of the previous week as a forecast for sales in the upcoming week. These predictions will be made for:\n",
    "* Total sales\n",
    "* Total sales per branch\n",
    "* Total sales per product\n",
    "* Sales for each product per branch\n",
    "\n",
    "We will use RMSE and MAE as our evaluation metric. Additionally, we will calculate the RMSE and MAE for overestimated and underestimated sales. For now, we will perform evaluation based on the whole time series.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/data_combined.csv')\n",
    "\n",
    "# parse 'date' to datetime object\n",
    "df.date = pd.to_datetime(df.date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate unstacked dataframe\n",
    "Currently, our dataframe represents a grouped time series (grouped by branch and product). For modeling, the dataframe will be unstacked (i.e. ungrouped), such that the individual time series are represented as separate columns in the dataframe. The ungrouping will be done for every single level of the grouped dataframe. Specifically, we will generate \n",
    "* a time series for total sales, summing up sales over all branches and products\n",
    "* summed time series for the three different bakery branches with sales summed up across products per branch\n",
    "* summed time series for the five different bakery products with sales summed up across branches per product\n",
    "* individual time series per branch and product\n",
    "\n",
    "In total, we will end up with 24 different time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unstack_time_series(df, index, groups, target):\n",
    "    \n",
    "    # create the individual combinations df\n",
    "    df_groups = df.pivot(index=index, columns=groups, values=target)\n",
    "    df_groups.columns = df_groups.columns.to_flat_index().map('{0[0]} | {0[1]}'.format)\n",
    "\n",
    "    # create df for first group\n",
    "    df_01 = df.groupby([index, groups[0]]) \\\n",
    "                        .sum() \\\n",
    "                        .reset_index(drop=False) \\\n",
    "                        .pivot(index=index, columns=groups[0], values=target)\n",
    "\n",
    "    # create df for second group\n",
    "    df_02 = df.groupby([index, groups[1]]) \\\n",
    "                        .sum() \\\n",
    "                        .reset_index(drop=False) \\\n",
    "                        .pivot(index=index, columns=groups[1], values=target)\n",
    "\n",
    "    # create the total level df\n",
    "    df_total = df.groupby(index)[target] \\\n",
    "                .sum() \\\n",
    "                .to_frame() \\\n",
    "                .rename(columns={target: 'total'})\n",
    "\n",
    "    # join the DataFrames\n",
    "    df_unstacked = df_total.join(df_01) \\\n",
    "                                .join(df_02) \\\n",
    "                                .join(df_groups)\n",
    "    df_unstacked.index = pd.to_datetime(df_unstacked.index)\n",
    "    return df_unstacked\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unstacked = unstack_time_series(df, 'date', ['Location', 'PredictionGroupName'], 'SoldTurnver')\n",
    "df_unstacked.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Modeling\n",
    "For baseline modeling, we simply shift the time series by seven days, thereby using the sales from days of the previous week as predictions for the sales on the respective days of the upcoming week. We calculate the residuals by simply subtracting the shifted time series from the actual time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions for original and min-max scaled data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "df_unstacked_scaled = pd.DataFrame(scaler.fit_transform(df_unstacked))\n",
    "df_unstacked_scaled.columns = df_unstacked.columns\n",
    "df_unstacked_scaled.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_by_previous_week(df):\n",
    "\n",
    "    # predict values by imputing sales from the day of the preceding week\n",
    "    df_pred = df.shift(7)\n",
    "\n",
    "    # calculate residuals\n",
    "    df_residual = df - df_pred\n",
    "\n",
    "    return df_pred, df_residual\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred, df_residual = predict_by_previous_week(df_unstacked)\n",
    "df_pred_scaled, df_residual_scaled = predict_by_previous_week(df_unstacked_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check missings due to shifting the time series\n",
    "df_residual.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some missings due to the shifting of the time series. In order to calculate the evaluation metrics, we first have to drop all rows with missings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_residual.dropna(inplace=True)\n",
    "df_residual_scaled.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluate performance\n",
    "We calculate the following main metrics for evaluating model performance: 1) RMSE, 2) MAE\n",
    "Additionally, we calculate RMSE and MAE both for predictions overestimating and underestimating sales. \n",
    "Finally, we also calculate the overall mean and standard deviation of the actual time series for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_eval_metrics(df_actual, df_residual):\n",
    "    \n",
    "    # iterate over all time series and calculate evaluation scores using list comprehension\n",
    "    mean_actual = [np.mean(df_actual[col]).round(4) for col in df_residual.columns]\n",
    "    std_actual = [np.std(df_actual[col]).round(4) for col in df_residual.columns]\n",
    "    rmse_total = [np.sqrt(np.mean(np.square(df_residual[col]))).round(4) for col in df_residual.columns]\n",
    "    mae_total = [np.mean(np.abs(df_residual[col])).round(4) for col in df_residual.columns]\n",
    "    rmse_over = [np.sqrt(np.mean(np.square(df_residual[df_residual[col]>0][col]))).round(4) for col in df_residual.columns]\n",
    "    rmse_under = [np.sqrt(np.mean(np.square(df_residual[df_residual[col]<=0][col]))).round(4) for col in df_residual.columns]\n",
    "    mae_over = [np.mean(df_residual[df_residual[col]>0][col]).round(4) for col in df_residual.columns]\n",
    "    mae_under = [np.mean(df_residual[df_residual[col]<=0][col]).round(4) for col in df_residual.columns]\n",
    "\n",
    "    # combine to dataframe\n",
    "    df_eval = pd.DataFrame({'groups': df_residual.columns, \n",
    "                            'mean_actual': mean_actual, 'std_actual': std_actual,\n",
    "                            'rmse_total': rmse_total, 'mae_total': mae_total,\n",
    "                            'rmse_over': rmse_over, 'rmse_under': rmse_under, 'mae_over': mae_over, 'mae_under': mae_under})\n",
    "    df_eval.set_index('groups', inplace=True, drop=True)\n",
    "    \n",
    "    # return evaluation metrics\n",
    "    return df_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### evaluation scores for predictions in original metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval = calculate_eval_metrics(df_unstacked, df_residual)\n",
    "df_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### evaluation scores for scaled data (min-max scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval_scaled = calculate_eval_metrics(df_unstacked_scaled, df_residual_scaled)\n",
    "df_eval_scaled"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.8 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5088fa60f7d34fb5f2ba3ff772c32280f8a6f8f3ea142d94c52ee17185bba4b2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
