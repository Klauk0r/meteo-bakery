{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meteo Bakery - Combine datasets\n",
    "This notebook serves to combine df_full data with the weather summary statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load df_full data\n",
    "sales = pd.read_excel('../data/neueFische_Umsaetze_Baeckerei.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data on engineered weather features\n",
    "weather_stats = pd.read_csv('../data/summary_stats.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load holidays data\n",
    "\n",
    "# school holidays from https://www.schulferien.org/oesterreich/ferien/2012/\n",
    "school_hols = pd.read_excel(\"../data/school_holidays.xlsx\")\n",
    "\n",
    "# public holidays from google search \"Feiertage Wien 'YEAR'\"\n",
    "public_hols = pd.read_excel(\"../data/public_holidays.xlsx\")\n",
    "public_hols.date = pd.to_datetime(public_hols.date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Corona data\n",
    "corona = pd.read_excel(\"../data/corona-measures-vienna.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering - Sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get basic information on datatypes and missings\n",
    "sales.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate location column based on branch\n",
    "# Filiale 1: U-Bahn\n",
    "# Filiale 2: Innenstadt\n",
    "# Filiale 3: Bahnhof\n",
    "\n",
    "sales['Branch'] = sales.Branch.apply(lambda x: 'Metro' if x==1 else 'Center' if x==2 else 'Train_Station')\n",
    "sales.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three missing values in the sales data ('SoldTurnver')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns\n",
    "sales.rename(columns={'Branch': 'branch', 'PredictionGroupName': 'product', 'SoldTurnver': 'turnover'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.rename(columns={'Date': 'date'}, inplace=True)\n",
    "sales.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#relabel products\n",
    "sales['product'] = sales['product'].map({'Mischbrote':'Brown Bread',\n",
    "                                'Weizenbrötchen':'Wheat Rolls',\n",
    "                                'klassischer Kuchen':'Cakes',\n",
    "                                'handliches Gebäck':'Pastries',\n",
    "                                'herzhafter Snack':'Savoury Snack'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count dates per branch and product category\n",
    "sales.groupby(['branch', 'product'])['date'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, not all dates are equally represented per group. This indicates that dates are not continuously progressing, but that there gaps present in the dates. Thus, there must be missing dates. Indeed, the first Covid19 lockdown has already removed from the data, representing one of possibly more gaps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a time series of consecutive dates as backbone\n",
    "To avoid such gaps, we will first generate a datetime column with consecutive gaps starting and ending with the first and last registered date. The other data will then be merged into that continuous date column, with gaps in certain columns being filled up with NaNs. These NaNs can be handled strategically during later analysis and modeling steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consec_dates = pd.DataFrame({'date':pd.date_range(sales.date.min(), sales.date.max())})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sales.date.nunique())\n",
    "print(consec_dates.date.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consec_dates.date.nunique() * 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### repeat the dates for each branch and product category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consec_dates[['Metro', 'Center', 'Train_Station']] = 'Metro', 'Center', 'Train_Station'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consec_dates.set_index('date', inplace=True)\n",
    "consec_dates.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consec_dates = consec_dates.stack().reset_index(name='branch').drop(columns=['level_1'])\n",
    "consec_dates.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "products = sales['product'].unique()\n",
    "consec_dates[products] = products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consec_dates.set_index(['date', 'branch'], inplace=True)\n",
    "consec_dates.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consec_dates = consec_dates.stack().reset_index(name='product').drop(columns=['level_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consec_dates.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consec_dates.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### merge sales into backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = consec_dates.merge(sales, on=['date', 'branch', 'product'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full.groupby(['branch', 'product'])['date'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full.date.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### append additional time information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract time features from Date column\n",
    "df_full['year'] = df_full.date.dt.year\n",
    "df_full['month'] = df_full.date.dt.month\n",
    "df_full['week'] = df_full.date.dt.week\n",
    "df_full['day_of_month'] = df_full.date.dt.day\n",
    "df_full['day_of_week'] = df_full.date.dt.dayofweek"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### append holiday and Covid information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append holidays by creating true/false columns\n",
    "df_full[\"school_holiday\"] = df_full[\"date\"].isin(school_hols[\"date\"])\n",
    "df_full[\"public_holiday\"] = df_full[\"date\"].isin(public_hols[\"date\"])\n",
    "\n",
    "# public holiday name\n",
    "df_full[\"p_hol_name\"] = df_full[\"public_holiday\"].copy()\n",
    "for x in range(public_hols.shape[0]):\n",
    "    df_full.loc[df_full[\"date\"] == public_hols.iloc[x, 0], \"p_hol_name\"] = public_hols.iloc[x, 1]\n",
    "# school holiday name\n",
    "df_full[\"s_hol_name\"] = df_full[\"school_holiday\"].copy()\n",
    "for x in range(school_hols.shape[0]):\n",
    "    df_full.loc[df_full[\"date\"] == school_hols.iloc[x, 0], \"s_hol_name\"] = school_hols.iloc[x, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# broadcast lockdown times\n",
    "df_full[\"lock\"] = 'open'\n",
    "df_full.loc[(df_full.date >= pd.to_datetime(\"2020-03-10\")) & (df_full.date < pd.to_datetime(\"2020-04-14\")),\"lock\"] = \"lockdown\"\n",
    "df_full.loc[(df_full.date >= pd.to_datetime(\"2020-11-03\")) & (df_full.date < pd.to_datetime(\"2020-11-17\")),\"lock\"] = \"lockdown_light\"\n",
    "df_full.loc[(df_full.date >= pd.to_datetime(\"2020-11-17\")) & (df_full.date < pd.to_datetime(\"2020-12-06\")),\"lock\"] = \"lockdown\"\n",
    "df_full.loc[(df_full.date >= pd.to_datetime(\"2020-12-26\")) & (df_full.date < pd.to_datetime(\"2021-02-07\")),\"lock\"] = \"lockdown\"\n",
    "df_full.loc[(df_full.date >= pd.to_datetime(\"2021-04-01\")) & (df_full.date < pd.to_datetime(\"2021-05-02\")),\"lock\"] = \"lockdown\"\n",
    "df_full.loc[(df_full.date >= pd.to_datetime(\"2021-11-08\")) & (df_full.date < pd.to_datetime(\"2021-12-31\")),\"lock\"] = \"lockdown\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deal with abrupt shifts in sales data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full[\"product\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for example, \"Savoury Snack\" in \"Metro\" has a sudden drop in sales after 2014 which is likely due to e.g. a change in categories or decrease in store size or something else which cannot be accounted for by a model\n",
    "plt.figure(figsize = (20,10))\n",
    "sns.scatterplot(data =df_full[(df_full[\"branch\"] == \"Metro\") & (df_full[\"product\"] == \"Savoury Snack\")], x = \"date\", y = \"turnover\")\n",
    "plt.xticks(rotation=45, horizontalalignment=\"right\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def mean_shift(df_full, branches, products, date1, date2, date3, date4, date5, date6):\n",
    "    \n",
    "    for br in branches:\n",
    "        for pr in products:\n",
    "            # multiplying with the respective mean shift compared to timeframe between date1 and date2 and date3 and date4\n",
    "            mean_before = df_full[\"turnover\"][(df_full[\"product\"] == pr) & (df_full[\"branch\"] == br) &\n",
    "            (((df_full[\"date\"] >= date1) & \n",
    "            (df_full[\"date\"] < date2)) | ((df_full[\"date\"] >= date3) & \n",
    "            (df_full[\"date\"] < date4)))].mean()\n",
    "            mean_of_interest = df_full[\"turnover\"][(df_full[\"product\"] == pr) & (df_full[\"branch\"] == br) &\n",
    "            (df_full[\"date\"] >= date5) & \n",
    "            (df_full[\"date\"] < date6)].mean()\n",
    "            df_full[\"turnover\"][(df_full[\"product\"] == pr) & (df_full[\"branch\"] == br) &\n",
    "            (df_full[\"date\"] >= date5) & \n",
    "            (df_full[\"date\"] < date6)] = df_full[\"turnover\"][(df_full[\"product\"] == pr) & (df_full[\"branch\"] == br) &\n",
    "            (df_full[\"date\"] >= date5) & \n",
    "            (df_full[\"date\"] < date6)] * mean_before / mean_of_interest\n",
    "    return df_full\n",
    "\n",
    "df_full = mean_shift(df_full, branches = [\"Metro\", \"Train_Station\", \"Center\"], products = [\"Wheat Rolls\"], \n",
    "date1 = pd.to_datetime(\"2016-05-01\"), date2 = pd.to_datetime(\"2016-07-30\"), \n",
    "date3 = pd.to_datetime(\"2017-05-01\"), date4 = pd.to_datetime(\"2017-07-30\"),\n",
    "date5 = pd.to_datetime(\"2018-05-01\"), date6 = pd.to_datetime(\"2018-07-30\"))\n",
    "\n",
    "df_full = mean_shift(df_full, branches = [\"Train_Station\"], products = [\"Wheat Rolls\"], \n",
    "date1 = pd.to_datetime(\"2016-01-01\"), date2 = pd.to_datetime(\"2016-12-31\"), \n",
    "date3 = pd.to_datetime(\"2017-01-01\"), date4 = pd.to_datetime(\"2017-12-31\"),\n",
    "date5 = pd.to_datetime(\"2012-07-01\"), date6 = pd.to_datetime(\"2015-12-31\"))\n",
    "\n",
    "df_full = mean_shift(df_full, branches = [\"Metro\"], products = [\"Brown Bread\"], \n",
    "date1 = pd.to_datetime(\"2016-01-01\"), date2 = pd.to_datetime(\"2016-12-31\"), \n",
    "date3 = pd.to_datetime(\"2017-01-01\"), date4 = pd.to_datetime(\"2017-12-31\"),\n",
    "date5 = pd.to_datetime(\"2018-08-01\"), date6 = pd.to_datetime(\"2020-03-11\"))\n",
    "\n",
    "df_full = mean_shift(df_full, branches = [\"Train_Station\"], products = [\"Savoury Snack\"], \n",
    "date1 = pd.to_datetime(\"2016-01-01\"), date2 = pd.to_datetime(\"2016-12-31\"), \n",
    "date3 = pd.to_datetime(\"2017-01-01\"), date4 = pd.to_datetime(\"2017-12-31\"),\n",
    "date5 = pd.to_datetime(\"2012-01-01\"), date6 = pd.to_datetime(\"2015-09-30\"))\n",
    "\n",
    "df_full = mean_shift(df_full, branches = [\"Train_Station\"], products = [\"Savoury Snack\"], \n",
    "date1 = pd.to_datetime(\"2016-10-01\"), date2 = pd.to_datetime(\"2016-12-31\"), \n",
    "date3 = pd.to_datetime(\"2017-10-01\"), date4 = pd.to_datetime(\"2017-12-31\"),\n",
    "date5 = pd.to_datetime(\"2015-10-01\"), date6 = pd.to_datetime(\"2015-12-31\"))\n",
    "\n",
    "df_full = mean_shift(df_full, branches = [\"Metro\"], products = [\"Savoury Snack\"], \n",
    "date1 = pd.to_datetime(\"2016-01-01\"), date2 = pd.to_datetime(\"2016-12-31\"), \n",
    "date3 = pd.to_datetime(\"2017-01-01\"), date4 = pd.to_datetime(\"2017-12-31\"),\n",
    "date5 = pd.to_datetime(\"2012-01-01\"), date6 = pd.to_datetime(\"2014-12-31\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after adjustment\n",
    "g = sns.FacetGrid(data = df_full, row = \"product\", col = \"branch\", aspect = 3, \n",
    "sharex = False, sharey = False)\n",
    "g.map(sns.scatterplot, \"date\", \"turnover\")\n",
    "plt.savefig(\"../plots/sales.overview.perbranchandproduct.adjusted.png\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### merge with weather statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_stats.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse date to datetime\n",
    "weather_stats['date'] = pd.to_datetime(weather_stats['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge dataframes\n",
    "df_joined = df_full.merge(weather_stats, on='date', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joined.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dummies for general weather\n",
    "# merging and cleaning condition column\n",
    "df_joined[\"general_w\"] = df_joined[\"condition_total\"]\n",
    "df_joined[\"general_w\"][df_joined[\"general_w\"] == \"['rainy' 'snowy']\"] = \"snowy\"\n",
    "df_joined[\"general_w\"][df_joined[\"general_w\"] == \"['clear' 'cloudy' 'rainy']\"] = \"rainy\"\n",
    "df_joined[\"general_w\"][df_joined[\"general_w\"] == \"['clear' 'cloudy']\"] = \"cloudy\"\n",
    "df_joined[\"general_w\"][df_joined[\"general_w\"] == \"['clear' 'sormy']\"] = \"stormy\"\n",
    "df_joined[\"general_w\"][df_joined[\"general_w\"] == \"['cloudy' 'rainy']\"] = \"rainy\"\n",
    "df_joined[\"general_w\"][df_joined[\"general_w\"] == \"['cloudy' 'snowy']\"] = \"snowy\"\n",
    "df_joined[\"general_w\"][df_joined[\"general_w\"] == \"['cloudy' 'rainy' 'snowy']\"] = \"snowy\"\n",
    "df_joined[\"general_w\"][df_joined[\"general_w\"] == \"['foggy' 'rainy']\"] = \"foggy\"\n",
    "df_joined[\"general_w\"][df_joined[\"general_w\"] == \"['clear' 'cloudy' 'snowy']\"] = \"snowy\"\n",
    "df_joined[\"general_w\"][df_joined[\"general_w\"] == \"['cloudy' 'rainy' 'stormy]\"] = \"stormy\"\n",
    "df_joined[\"general_w\"][df_joined[\"general_w\"] == \"['clear' 'foggy']\"] = \"foggy\"\n",
    "df_joined[\"general_w\"][df_joined[\"general_w\"] == \"['clear' 'cloudy' 'foggy']\"] = \"foggy\"\n",
    "df_joined[\"general_w\"][df_joined[\"general_w\"] == \"['clear' 'rainy']\"] = \"rainy\"\n",
    "df_joined[\"general_w\"][df_joined[\"general_w\"] == \"['cloudy' 'foggy' 'rainy']\"] = \"foggy\"\n",
    "df_joined[\"general_w\"][df_joined[\"general_w\"] == \"['foggy' 'snowy']\"] = \"foggy\"\n",
    "df_joined[\"general_w\"][df_joined[\"general_w\"] == \"['clear' 'stormy']\"] = \"stormy\"\n",
    "df_joined[\"general_w\"][df_joined[\"general_w\"] == \"['cloudy' 'foggy']\"] = \"foggy\"\n",
    "df_joined[\"general_w\"][df_joined[\"general_w\"] == \"['cloudy' 'rainy' 'stormy']\"] = \"stormy\"\n",
    "df_joined[\"general_w\"].unique()\n",
    "# for the moment I just fill NaNs with previous value. \n",
    "df_joined[\"general_w\"].fillna(method=\"ffill\", inplace=True)\n",
    "# then dummies for general weather\n",
    "dummies = pd.get_dummies(df_joined['general_w'], prefix=\"general_w\")\n",
    "df_joined = df_joined.join(dummies)\n",
    "\n",
    "# column for weekend or not\n",
    "df_joined[\"weekend\"] = df_joined.day_of_week.isin([5, 6]).astype(\"float\")\n",
    "\n",
    "# create dummies for school holidays\n",
    "dummies = pd.get_dummies(df_joined['s_hol_name'], prefix=\"sh\")\n",
    "df_joined = df_joined.join(dummies)\n",
    "\n",
    "# and dummies for public holidays\n",
    "dummies = pd.get_dummies(df_joined['p_hol_name'], prefix=\"ph\")\n",
    "df_joined = df_joined.join(dummies)\n",
    "\n",
    "# get temperature change from one to another day\n",
    "tmp_temp = df_joined.temp_mean.copy()\n",
    "tmp_temp_day_before = np.array(tmp_temp[15:]) - np.array(tmp_temp[:-15])\n",
    "tmp_temp = list(chain.from_iterable([list(np.repeat(\"NaN\",15)), list(tmp_temp_day_before)]))\n",
    "df_joined[\"temp_shift\"] = tmp_temp\n",
    "\n",
    "# get pressure change as proxy for change of weather\n",
    "tmp_pr = df_joined.pressure_mean.copy()\n",
    "tmp_pr_day_before = np.array(tmp_pr[15:]) - np.array(tmp_pr[:-15])\n",
    "tmp_pr = list(chain.from_iterable([list(np.repeat(\"NaN\",15)), list(tmp_pr_day_before)]))\n",
    "df_joined[\"pressure_shift\"] = tmp_pr\n",
    "\n",
    "# tomorrows weather\n",
    "tmp_weather = df_joined[\"general_w\"].copy()\n",
    "tmp_next_day = list(chain.from_iterable([list(tmp_weather[15:]), list(np.repeat(\"NaN\",15))]))\n",
    "df_joined[\"tomorrows_weather\"] = tmp_next_day\n",
    "\n",
    "# then dummies for tomorrows weather\n",
    "dummies = pd.get_dummies(df_joined['tomorrows_weather'], prefix=\"tw\")\n",
    "df_joined = df_joined.join(dummies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export combined data to csv file\n",
    "df_joined.to_csv('../data/data_combined.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.8 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b74997555c8f91a3719447544a5e0eea52b5cd1d12edaef9a97be210534824f7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
