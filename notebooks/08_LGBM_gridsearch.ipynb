{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meteo Bakery - LightGBM feature importance\n",
    "In this notebook, we will test LightGBM algorithm and assess feature importance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from itertools import product\n",
    "\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "from lightgbm import LGBMRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/data_combined.csv')\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df.set_index('date', inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transform periodic month feature using sine and cosine functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['month_sin'] = df.month.apply(lambda x: np.sin(np.array(x) * np.pi /6))\n",
    "df['month_cos'] = df.month.apply(lambda x: np.cos(np.array(x) * np.pi /6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### select only years up to 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.year<2020]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate lag features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize empty dataframe\n",
    "df_lag = pd.DataFrame({})\n",
    "for i, group in enumerate(product(df['branch'].unique(), df['product'].unique())):\n",
    "        # subselect time series and generate lag features\n",
    "        ts = df[(df['branch']==group[0]) & (df['product']==group[1])].copy()\n",
    "        # map feature to dictionary\n",
    "        target_map = ts['turnover'].to_dict()\n",
    "        ts['lag_07'] = (ts.index - pd.Timedelta('7 days')).map(target_map)\n",
    "        ts['lag_365'] = (ts.index - pd.Timedelta('365 days')).map(target_map)\n",
    "        # concatenate to dataframe\n",
    "        df_lag = pd.concat([df_lag, ts], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lag.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lag.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check missings in sales data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lag.groupby(['branch', 'product'])['turnover', 'month'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lag[(df_lag['turnover'].isnull()) & (df_lag['branch']=='Metro')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 missing days for Metro station. Additionally, there no sales for Mischbrote on 16-10-2018 and 16-10-2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lag[(df_lag['turnover'].isnull()) & (df_lag['branch']=='Train_Station')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Station has exactly the same missings as Metro branch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lag[(df_lag['turnover'].isnull()) & (df_lag['branch']=='Center') & (df_lag['product']=='Mischbrote')].head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "69 missing days for Center branch. There frequently fall on a public holiday, thus indicating that this branch probably had closed on these days."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### replace NaNs\n",
    "replace NaNs in turnover column with turnover of corresponding day of preceding weak, otherwise, use combined forward fill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_repl = df_lag.copy()\n",
    "# replace NaN at Center branch by 1 is occurring on public holiday\n",
    "df_repl.loc[(df_repl['branch']=='Center') & (df_repl['public_holiday']==True), 'turnover'] = df_repl.loc[(df_repl['branch']=='Center') & (df_repl['public_holiday']==True), 'turnover'].fillna(1)\n",
    "\n",
    "# fill NaN with sales from previous day of week\n",
    "df_repl['turnover'] = df_repl['turnover'].fillna(df_repl['lag_07'])\n",
    "\n",
    "# fill remaining NaN using forward fill\n",
    "for i, group in enumerate(product(df_repl['branch'].unique(), df_repl['product'].unique())):\n",
    "        df_repl[(df['branch']==group[0]) & (df_repl['product']==group[1])].ffill(inplace=True, axis='rows')\n",
    "\n",
    "df_repl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_repl.loc[(df_repl['branch']=='Metro') & (df_repl['product']=='Mischbrote'), ['branch', 'product', 'turnover', 'lag_07']].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_repl.groupby(['branch', 'product'])[['turnover', 'lag_07', 'month']].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_repl[(df_repl['public_holiday']==True) & (df_repl['branch']=='Center')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate train and test df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_repl[df_repl.year<2018]\n",
    "df_test = df_repl[df_repl.year>=2018]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Series Split Cross Validation\n",
    "Number of splits is set to 52 and test sizeto 7 days, thus representing a whole year. For now, we won't introduce any gab between training and validation set. However, assuming that bakery stores usually make their demand planning for the upcoming week on certain days of the week, e.g. Thursdays, we may later additionally introduce a gap of 3 days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract an example time series for illustration purposes and perform TimeSeriesSplit\n",
    "ts = df_train[(df_train['branch']=='Metro') & (df_train['product']=='Mischbrote')]['turnover']\n",
    "tss = TimeSeriesSplit(n_splits=52, test_size=7, gap=3)\n",
    "\n",
    "fold=0\n",
    "# plot repeated train-validation folds to get an idea of TimeSeriesSplit functionality\n",
    "for train_i, val_i in tss.split(ts):\n",
    "    ts_train = ts.iloc[train_i]\n",
    "    ts_val = ts.iloc[val_i]\n",
    "\n",
    "    plt.figure(figsize=(10, 1))\n",
    "    ts_train[-500:].plot(c='blue', label='training')\n",
    "    ts_val.plot(c='red', label='validation')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Seasonal baseline\n",
    "We will first define a utility function to perform cross-validation on naive seasonal baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### use sales 14 days ago if prediction contains missing from closed Center branch on public holidays\n",
    "It is possible that a prediction for Naive Seasonal baseline includes values == 1 as prediction, which relfected NaN from public holidays in the preceding 7 day-interval used for prediction in case of Center branch.\n",
    "\n",
    "In such as case, replace any NaN (currently encoded as 1) by the turnover day from exactly 7 days ago. Thus, in case a holiday is contained in the preceding week before prediction, any such holiday is replaced by sales data from the day of the preceding week. Thus, not the sales 7 days before before are used as a prediction when falling on a holiday, but instead the sales 14 days ago.\n",
    "\n",
    "#### also correct validation set if necessary\n",
    "Find missings in validation set due to holiday-related closing (represented as 1), extract index positions and drop elements at these index positions from both validation and training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function for naive seasonal baseline corrected for holiday effects\n",
    "def naive_seasonal_cv_holiday(df_train, grouping_features, target, gap=0):\n",
    "    # initialize dataframe for evaluation scores\n",
    "    cval = pd.DataFrame({'group': [], 'MAPE': []})\n",
    "\n",
    "    # iterate over all individual series and perform cross-validation\n",
    "    from itertools import product\n",
    "    for i, group in enumerate(product(df_train[grouping_features[0]].unique(), df_train[grouping_features[1]].unique())):\n",
    "\n",
    "        # subselect time series\n",
    "        ts = df_train[(df_train[grouping_features[0]]==group[0]) & (df_train[grouping_features[1]]==group[1])].copy()\n",
    "\n",
    "        # perform cross validation\n",
    "        tss = TimeSeriesSplit(n_splits=52, test_size=7, gap=gap)\n",
    "\n",
    "        scores = []\n",
    "        for train_i, val_i in tss.split(ts):\n",
    "\n",
    "            target = 'turnover'\n",
    "            y_train = ts.iloc[train_i][target]\n",
    "            y_val = ts.iloc[val_i][target]\n",
    "            \n",
    "            # correct for holiday effects in predicted values based on training set if necessary\n",
    "            # if holiday is in prediced y-values, replace by sales 14 days ago\n",
    "            if 1 in y_train[-7:].unique():\n",
    "                idx_train = [i for i in range(len(y_train[-7:].tolist())) if y_train[-7:].tolist()[i]==1]\n",
    "                idx_train = [i-7 for i in idx_train]\n",
    "                idx_train_lag = [i-7 for i in idx_train]\n",
    "                y_train_repl = y_train.copy()\n",
    "                y_train_repl.iloc[idx_train] = [x for x in y_train.iloc[idx_train_lag]]\n",
    "                y_pred = y_train_repl[-7:]\n",
    "            else:\n",
    "                y_pred = y_train[-7:]\n",
    "\n",
    "            # correct for holiday effects in validation set if necessary\n",
    "            # if holiday is in validation set, drop elements at corresponding index position in both y_val and y_pred\n",
    "            if 1 in y_val.unique():\n",
    "                idx_val = [i for i in range(len(y_val.tolist())) if y_val.tolist()[i]==1]\n",
    "                y_val = y_val.drop(y_val.index[idx_val])\n",
    "                y_pred = y_pred.drop(y_pred.index[idx_val])\n",
    "\n",
    "            mape = mean_absolute_percentage_error(y_val, y_pred)\n",
    "            scores.append(mape)\n",
    "        \n",
    "        # append scores\n",
    "        cval.loc[i, 'group'] = f'{group[0]} | {group[1]}'\n",
    "        cval.loc[i, 'MAPE'] = np.mean(scores)\n",
    "    # calculate mean scores\n",
    "    cval.loc[i+1, 'group'] = 'mean'\n",
    "    cval.loc[i+1, 'MAPE'] = cval['MAPE'].mean()\n",
    "\n",
    "    return cval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_naive = naive_seasonal_cv_holiday(df_train, grouping_features=['branch', 'product'], target='turnover')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_naive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM\n",
    "We will iterate over every time series and evaluate LightGBM performance with only temporal and additional weather features using TimeSeriesSplit Cross-Validation.\n",
    "\n",
    "First, we will define a utility function performing Cross-Validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function for LightGBM\n",
    "def LGBM_cv(df_train, grouping_vars, target, features, gap=0):\n",
    "    # initialize dataframe for evaluation scores\n",
    "    cval = pd.DataFrame({'group': [], 'MAPE': []})\n",
    "\n",
    "    # iterate over all individual series and perform cross-validation\n",
    "    from itertools import product\n",
    "    for i, group in enumerate(product(df_train[grouping_vars[0]].unique(), df_train[grouping_vars[1]].unique())):\n",
    "\n",
    "        # subselect time series\n",
    "        ts = df_train[(df_train[grouping_vars[0]]==group[0]) & (df_train[grouping_vars[1]]==group[1])].copy()\n",
    "\n",
    "        # perform cross validation\n",
    "        tss = TimeSeriesSplit(n_splits=52, test_size=7, gap=gap)\n",
    "\n",
    "        scores = []\n",
    "        for train_i, val_i in tss.split(ts):\n",
    "\n",
    "            train = ts.iloc[train_i]\n",
    "            val = ts.iloc[val_i]\n",
    "                    \n",
    "            X_train = train[features]\n",
    "            X_val= val[features]\n",
    "            y_train = train[target]\n",
    "            y_val = val[target]\n",
    "\n",
    "            lgbm = LGBMRegressor(objective='regression', random_state=42)\n",
    "            lgbm.fit(X_train, y_train)\n",
    "\n",
    "            y_pred = pd.Series(lgbm.predict(X_val))\n",
    "\n",
    "            # correct for holiday effects in validation set if necessary\n",
    "            # if holiday is in validation set, drop elements at corresponding index position in both y_val and y_pred\n",
    "            if 1 in y_val.unique():\n",
    "                idx_val = [i for i in range(len(y_val.tolist())) if y_val.tolist()[i]==1]\n",
    "                y_val = y_val.drop(y_val.index[idx_val])\n",
    "                y_pred = y_pred.drop(y_pred.index[idx_val])\n",
    "\n",
    "            mape = mean_absolute_percentage_error(y_val, y_pred)\n",
    "            scores.append(mape)\n",
    "        \n",
    "        # append scores\n",
    "        cval.loc[i, 'group'] = f'{group[0]} | {group[1]}'\n",
    "        cval.loc[i, 'MAPE'] = np.mean(scores)\n",
    "    # calculate mean scores\n",
    "    cval.loc[i+1, 'group'] = 'mean'\n",
    "    cval.loc[i+1, 'MAPE'] = [cval['MAPE'].mean()]\n",
    "    \n",
    "    return cval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### temporal features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_features = ['lag_07', 'lag_365', 'month_sin', 'month_cos', 'day_of_week', 'school_holiday', 'public_holiday']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_lgbm_time = LGBM_cv(df_train, grouping_vars=['branch', 'product'], target='turnover', features=time_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_lgbm_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### basic weather statistics (temperature, humidity, rain, snow)\n",
    "Here, will will add daily weather aggregate features as predictors to assess any add-on effect in addition to the temporal features. We will use mean temperature, humidity, rain, and snow as weather features since they appear most promising based on previous EDA results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_weather_features = ['lag_07', 'lag_365', 'month_sin', 'month_cos', 'day_of_week', 'school_holiday', 'public_holiday',\n",
    "                                    'temp_mean', 'humidity_mean', 'rain_1h_mean', 'snow_1h_mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_lgbm_weather = LGBM_cv(df_train, grouping_vars=['branch', 'product'], target='turnover', features=mean_weather_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_lgbm_weather"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### basic weather statistics and climatological days\n",
    "In addition to aggregated features, we calculated features for climatological days according to [DWD](https://www.dwd.de/DE/service/lexikon/Functions/glossar.html;jsessionid=EB2D3A27D634826A0176255436956DA7.live21064?lv2=101334&lv3=101452) based on our weather statistics. We first performed some basic EDA to test which climatological days could serve as potential predictors in the LGBM model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### visualize relative occurrence of climatological days depending on month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for day in ['day_icy', 'day_frosty', 'day_thunder', 'day_hot', 'day_clear','day_hazy', 'day_rainy', 'day_summer', 'day_murky']:\n",
    "    plt.figure(figsize=(7,1))\n",
    "    sns.barplot(data=df, y=day, x='month', color='white', edgecolor='blue')\n",
    "    plt.yticks(ticks=np.arange(0, 0.81, 0.2));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### visualize differences in sales for different product categories depending on climatological days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for day in ['day_icy', 'day_frosty', 'day_thunder', 'day_hot', 'day_clear','day_hazy', 'day_rainy', 'day_summer', 'day_murky']:\n",
    "    plt.figure(figsize=(5, 2))\n",
    "    sns.barplot(data=df, x='product', y='turnover', edgecolor='blue', hue=day)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.legend(title=day, bbox_to_anchor=(1.05, 1.0), loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Day frosty and Day icy measure similar weather conditions and have similar effects. The same holds true for day hot and day summer. However, day frosty and day summer have higher occurrence, so we will use these ones as predictors, as opposed to the other ones. \n",
    "\n",
    "Day rainy has almost no occurrence and almost no effect and is therefore not used as predictor. Day murky is also not used since it doesn´t have any clear effect and represents the counterpart to day clear. \n",
    "\n",
    "Day clear and day hazy don´t seem to have clear effects either, but are included as predictors at this stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_weather_climat_features = ['lag_07', 'lag_365', 'month_sin', 'month_cos', 'day_of_week', 'school_holiday', 'public_holiday',\n",
    "                                    'temp_mean', 'humidity_mean', 'rain_1h_mean', 'snow_1h_mean', 'day_frosty', 'day_thunder', 'day_clear','day_hazy', 'day_summer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_lgbm_climat = LGBM_cv(df_train, grouping_vars=['branch', 'product'], target='turnover', features=mean_weather_climat_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_lgbm_climat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### basic weather statistics, climatological days and elative frequency measures of weather states\n",
    "Finally, we will also test whether the inclusion of relative frequency of defined weather states affects forecasting performance. To this end, we will first perform some feature pre-selection based on basic EDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### visualize differences in sales for different product categories depending relative frequencies of weather states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in ['clear_total', 'cloudy_total', 'foggy_total', 'rainy_total', 'snowy_total', 'thunderstorm_total', 'tornado_total']:\n",
    "    fig, ax = plt.subplots(figsize=(9, 2))\n",
    "    from matplotlib.ticker import FormatStrFormatter\n",
    "    ax.xaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n",
    "    sns.barplot(data=df, x=x, y='turnover', edgecolor='blue', hue='product', ax=ax)\n",
    "    xlabels = ['{:,.2f}'.format(x) for x in ax.get_xticks()]\n",
    "    ax.set_xticklabels(xlabels, rotation=45)\n",
    "    plt.legend(fontsize=7, bbox_to_anchor=(1.05, 1.0), loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no clear effects of either of the relative frequency statistics. However, we will still feed them in as predictors to check if they have any additional effect. Thunderstorm and Tornado will be excluded, since they barely occur at all and Thunderstorm is already encoded as a day_thunder for the climatological days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_weather_climat_rel_features = ['lag_07', 'lag_365', 'month_sin', 'month_cos', 'day_of_week', 'school_holiday', 'public_holiday',\n",
    "                                    'temp_mean', 'humidity_mean', 'rain_1h_mean', 'snow_1h_mean', 'day_frosty', 'day_thunder', 'day_clear','day_hazy', 'day_summer',\n",
    "                                    'clear_total', 'cloudy_total', 'foggy_total', 'rainy_total', 'snowy_total']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_lgbm_climat_rel = LGBM_cv(df_train, grouping_vars=['branch', 'product'], target='turnover', features=mean_weather_climat_rel_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_lgbm_climat_rel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further including relative frequencies of weather states slightly improved the forecasting performance. We will also test the forecasting performance of LGBM model when only including basic aggregate weather statistics and relative frequencies (w/o climatological days)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### only basic weather statistics and relative frequency measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_weather_rel_features = ['lag_07', 'lag_365', 'month_sin', 'month_cos', 'day_of_week', 'school_holiday', 'public_holiday',\n",
    "                                    'temp_mean', 'humidity_mean', 'rain_1h_mean', 'snow_1h_mean',\n",
    "                                    'clear_total', 'cloudy_total', 'foggy_total', 'rainy_total', 'snowy_total']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_lgbm_freq = LGBM_cv(df_train, grouping_vars=['branch', 'product'], target='turnover', features=mean_weather_rel_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_lgbm_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relative frequencies of weather states worsen the prediction when fed into cross-validation.\n",
    "\n",
    "We will therefore continue with the daily mean weather statistics and climatological days, since these features resulted in the best performance on cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch on LightGBM\n",
    "After identifying the best combination of features, we will perform hyperparameter tuning using GridSearch to further optimize LGBM forecasting performance. Specifically, we will test different boosting types, numbers of estimators, and different learning rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create hyperparameter dictionary for grid search\n",
    "lgbm_params = {\n",
    "    'boosting_type': ['gbdt', 'dart'],\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.05, 0.08, 0.1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize df for storing results from gridsearch\n",
    "grid_results = pd.DataFrame({'boosting_type': [], 'n_estimators': [], 'learning_rate': [], 'MAPE': []})\n",
    "\n",
    "for i, params in enumerate(product(lgbm_params['boosting_type'], lgbm_params['n_estimators'], lgbm_params['learning_rate'])):\n",
    "    print(params)\n",
    "\n",
    "    local_params = {\n",
    "        'boosting_type': params[0],\n",
    "        'n_estimators': int(params[1]),\n",
    "        'learning_rate': float(params[2])}\n",
    "    \n",
    "                \n",
    "    lgbm = LGBMRegressor(objective='regression', importance_type='gain', random_state=42, **local_params)\n",
    "    \n",
    "    # initialize empty list to compute average MAPE overall individual time series per hyperparameter configuration\n",
    "    mapes_local = []\n",
    "\n",
    "     # iterate over all individual series and perform cross-validation\n",
    "    for k, group in enumerate(product(df_train['branch'].unique(), df_train['product'].unique())):\n",
    "        # subselect time series and generate lag features\n",
    "        ts = df_train[(df_train['branch']==group[0]) & (df_train['product']==group[1])].copy()\n",
    "\n",
    "        # perform cross validation\n",
    "        tss = TimeSeriesSplit(n_splits=52, test_size=7, gap=0)\n",
    "\n",
    "        fold=0\n",
    "        scores = []\n",
    "        for train_i, val_i in tss.split(ts):\n",
    "\n",
    "            train = ts.iloc[train_i]\n",
    "            val = ts.iloc[val_i]\n",
    "            features = ['lag_07', 'lag_365', 'month_sin', 'month_cos', 'day_of_week', 'school_holiday', 'public_holiday',\n",
    "                            'temp_mean', 'humidity_mean', 'rain_1h_mean', 'snow_1h_mean',\n",
    "                                                    'day_frosty', 'day_thunder', 'day_clear','day_hazy', 'day_summer']\n",
    "                    \n",
    "            X_train = train[features]\n",
    "            X_val = val[features]\n",
    "            y_train = train['turnover']\n",
    "            y_val = val['turnover']\n",
    "\n",
    "            lgbm.fit(X_train, y_train)\n",
    "            y_pred = pd.Series(lgbm.predict(X_val))\n",
    "            # correct for holiday effects in validation set if necessary\n",
    "            # if holiday is in validation set, drop elements at corresponding index position in both y_val and y_pred\n",
    "            if 1 in y_val.unique():\n",
    "                idx_val = [i for i in range(len(y_val.tolist())) if y_val.tolist()[i]==1]\n",
    "                y_val = y_val.drop(y_val.index[idx_val])\n",
    "                y_pred = y_pred.drop(y_pred.index[idx_val])\n",
    "\n",
    "            mape = mean_absolute_percentage_error(y_val, y_pred)\n",
    "            scores.append(mape)\n",
    "        \n",
    "        # calculate mean MAPE score for individual time series\n",
    "        mean_score = np.mean(scores)\n",
    "        mapes_local.append(mean_score)\n",
    "    \n",
    "    grid_results.loc[i, 'boosting_type'] = params[0]\n",
    "    grid_results.loc[i, 'n_estimators'] = params[1]\n",
    "    grid_results.loc[i, 'learning_rate'] = params[2]\n",
    "    grid_results.loc[i, 'MAPE'] = np.mean(mapes_local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_results.to_csv('../models/LGBM_hyperparams.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.8 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b74997555c8f91a3719447544a5e0eea52b5cd1d12edaef9a97be210534824f7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
