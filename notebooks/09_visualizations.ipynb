{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meteo Bakery - Presentation Figures\n",
    "In this notebook, we will generate figures for presentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import meteo_utils as meteo\n",
    "\n",
    "from itertools import product\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load cross-validation results\n",
    "scores_merged = pd.read_csv('../models/lgbm_optimized/cross_validation.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stack data to prepare for visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stack and group according to model\n",
    "scores_grouped = pd.DataFrame(scores_merged[['group', 'MAPE_mean_naive', 'MAPE_mean_lgbm_time', 'MAPE_mean_lgbm_weather']].set_index('group').stack().reset_index().iloc[:-3, :])\n",
    "scores_grouped.columns = ['group', 'model', 'MAPE_mean']\n",
    "scores_grouped['MAPE_std'] = pd.DataFrame(scores_merged[['group',  'MAPE_std_naive', 'MAPE_std_lgbm_time', 'MAPE_std_lgbm_weather']].set_index('group').stack().reset_index().iloc[:-3, :])[0]\n",
    "scores_grouped['model'] = [x.split('_')[-1] for x in scores_grouped['model']]\n",
    "\n",
    "# multiply with 100 to get MAPE scores in %\n",
    "scores_grouped[['MAPE_mean', 'MAPE_std']] = scores_grouped[['MAPE_mean', 'MAPE_std']] *100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract branch and product information as separate columns\n",
    "scores_grouped['branch'] = [x.split(' | ')[0] for x in scores_grouped['group']]\n",
    "scores_grouped['product'] = [x.split(' | ')[1] for x in scores_grouped['group']]\n",
    "scores_grouped.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot mean MAPE and standard deviation from cross-validation over all groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(3,3))\n",
    "sns.barplot(data=scores_grouped, x='model', y='MAPE_mean', color='#2aa2cc', edgecolor='black', ci=None)\n",
    "plt.ylabel('Average prediction error [%]', fontsize=12)\n",
    "plt.yticks(np.arange(0, 26, 5), fontsize=11)\n",
    "plt.xlabel(None)\n",
    "plt.xticks(ticks=np.arange(0, 3), labels=['Baseline', 'LightGBM time', 'LightGBM weather'], fontsize=12, rotation=45, ha='right');\n",
    "plt.title('Model Comparison', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(3,3))\n",
    "sns.barplot(data=scores_grouped, x='model', y='MAPE_std', color='#2aa2cc', edgecolor='black', ci=None)\n",
    "plt.ylabel('Variability of prediction error [%]', fontsize=12)\n",
    "plt.yticks(np.arange(0, 26, 5), fontsize=11)\n",
    "plt.xlabel(None)\n",
    "plt.xticks(ticks=np.arange(0, 3), labels=['Baseline', 'LightGBM time', 'LightGBM weather'], fontsize=12, rotation=45, ha='right');\n",
    "plt.title('Model Comparison', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot scores separaly for each branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(4,3))\n",
    "sns.barplot(data=scores_grouped, x='branch', y='MAPE_mean', edgecolor='black', ci=None, hue='model', palette=['#d6633a', '#34831B', '#1b6883'],\n",
    "                    order=['Metro', 'Train_Station', 'Center'])\n",
    "plt.ylabel('Average prediction error [%]', fontsize=12)\n",
    "plt.yticks(np.arange(0, 26, 5), fontsize=11)\n",
    "plt.xlabel(None)\n",
    "plt.xticks(ticks=np.arange(0, 3), labels=['Metro', 'Train Station', 'Center'], fontsize=12, rotation=45, ha='right');\n",
    "plt.title('Model Comparison', fontsize=14)\n",
    "leg= plt.legend(bbox_to_anchor=(1.01, 0.4), loc='upper left', frameon=False, fontsize=11)\n",
    "leg.get_texts()[0].set_text('Baseline')\n",
    "leg.get_texts()[1].set_text('LightGBM time')\n",
    "leg.get_texts()[2].set_text('LightGBM weather')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(4,3))\n",
    "sns.barplot(data=scores_grouped, x='branch', y='MAPE_std', edgecolor='black', ci=None, hue='model', palette=['#d6633a', '#34831B', '#1b6883'],\n",
    "                    order=['Metro', 'Train_Station', 'Center'])\n",
    "plt.ylabel('Variability of prediction error [%]', fontsize=12)\n",
    "plt.yticks(np.arange(0, 26, 5), fontsize=11)\n",
    "plt.xlabel(None)\n",
    "plt.xticks(ticks=np.arange(0, 3), labels=['Metro', 'Train Station', 'Center'], fontsize=12, rotation=45, ha='right');\n",
    "plt.title('Model Comparison', fontsize=14)\n",
    "leg= plt.legend(bbox_to_anchor=(1.01, 0.4), loc='upper left', frameon=False, fontsize=11)\n",
    "leg.get_texts()[0].set_text('Baseline')\n",
    "leg.get_texts()[1].set_text('LightGBM time')\n",
    "leg.get_texts()[2].set_text('LightGBM weather')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_fimportance_rel = pd.read_csv('../models/lgbm_optimized/rel_feature_importance.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stack data to prepare for visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stack dataframe and group according to feature\n",
    "lgbm_fi_stacked = lgbm_fimportance_rel.set_index('group').stack().reset_index()\n",
    "lgbm_fi_stacked.columns = ['group', 'features', 'importance']\n",
    "lgbm_fi_stacked = lgbm_fi_stacked[lgbm_fi_stacked['group']!='mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract branch and product information as separate columns\n",
    "lgbm_fi_stacked['branch'] = [x.split(' | ')[0] for x in lgbm_fi_stacked['group']]\n",
    "lgbm_fi_stacked['product'] = [x.split(' | ')[1] for x in lgbm_fi_stacked['group']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_fi_stacked.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract features ordered by the relative feature importance\n",
    "col_order = lgbm_fimportance_rel.set_index('group').sort_values(by='mean', axis=1, ascending=False).columns\n",
    "col_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create feature map for renaming features during plotting\n",
    "feature_map = {'turnover_lag_7':'turnover [lag 7]', \n",
    "                'day_of_week':'day of week', \n",
    "                'public_holiday': 'public holiday', \n",
    "                'turnover_lag_365': 'turnover [lag 365]',\n",
    "                'temp_mean': 'temperature [daily mean]', \n",
    "                'snow_1h_mean_dev': 'snowfall [season. dev.]', \n",
    "                'month_cos': 'month [cosine-t.]', \n",
    "                'school_holiday': 'school holiday',\n",
    "                'temp_mean_dev': 'temperature [season. dev.]', \n",
    "                'temp_mean_lead_1': 'temperature [next day]', \n",
    "                'month_sin': 'month [sine-t.]', \n",
    "                'rain_1h_mean_dev': 'rainfall [season. dev.]',\n",
    "                'humidity_mean': 'humidity [daily mean]', \n",
    "                'pressure_mean_dev': 'atm. pressure [season. dev.]', \n",
    "                'humidity_mean_dev': 'humidity [season. dev.]',\n",
    "                'pressure_mean_change': 'atm. pressure [change]', \n",
    "                'temp_mean_change': 'temperature [change]', \n",
    "                'humidity_mean_change': 'humidity [change]',\n",
    "                'rain_1h_mean': 'rainfall [daily mean]', \n",
    "                'rain_1h_mean_lead_1': 'rainfall [next day]', \n",
    "                'day_hazy': 'hazy day', \n",
    "                'day_clear': 'clear day',\n",
    "                'day_frosty': 'frosty day', \n",
    "                'day_summer': 'summer day', \n",
    "                'snow_1h_mean_lead_1': 'snowfall [next day]', \n",
    "                'snow_1h_mean': 'snowfall [daily mean]',\n",
    "                'day_thunder': 'thunder day'\n",
    "                }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot global feature importance averaged over all groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(7, 6))\n",
    "sns.barplot(data=lgbm_fi_stacked, y='features', x='importance', color='#2aa2cc', edgecolor='black', ci=None, order=col_order)\n",
    "plt.xlabel('Relative Importance [%]', fontsize=12)\n",
    "plt.xticks(ticks=np.arange(0, 51, 10), labels=np.arange(0, 51, 10), fontsize=11)\n",
    "plt.ylabel(None)\n",
    "plt.yticks(ticks=np.arange(0, 27), labels=col_order.map(feature_map), fontsize=10)\n",
    "plt.title('Feature Importance', fontsize=14)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot top 6 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(7, 3))\n",
    "#fig.patch.set_visible(False)\n",
    "sns.barplot(data=lgbm_fi_stacked[lgbm_fi_stacked['importance']>2], y='features', x='importance', color='#2aa2cc', \n",
    "                    edgecolor='black', ci=None, order=col_order[:6])\n",
    "plt.xlabel('Relative Importance [%]', fontsize=13)\n",
    "plt.xticks(ticks=np.arange(0, 52, 10), labels=np.arange(0, 52, 10), fontsize=11)\n",
    "plt.ylabel(None)\n",
    "plt.yticks(ticks=np.arange(0, 6), labels=col_order[:6].map(feature_map), fontsize=11)\n",
    "plt.title('Feature Importance (Top 6)', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot feature importance of weather features only separately for each product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for product in lgbm_fi_stacked['product'].unique():\n",
    "    fimp = lgbm_fimportance_rel.copy()\n",
    "    fimp.drop(['turnover_lag_7', 'turnover_lag_365', 'month_sin', 'month_cos', 'day_of_week', 'school_holiday', 'public_holiday'], \n",
    "                    axis=1, inplace=True)\n",
    "    feature_cols = fimp.columns[1:]\n",
    "    fimp.loc[:14, 'branch'] = [x.split(' | ')[0] for x in fimp.loc[:14, 'group']]\n",
    "    fimp.loc[:14, 'product'] = [x.split(' | ')[1] for x in fimp.loc[:14, 'group']]\n",
    "    fimp = fimp[fimp['product']==product]\n",
    "    fimp.loc[3, 'group'] = 'mean'\n",
    "    fimp.loc[3, feature_cols] = [np.mean(fimp[x]) for x in feature_cols]\n",
    "    temp_order = fimp[fimp.columns[:-2]].set_index('group').sort_values(by='mean', axis=1, ascending=False).columns\n",
    "    \n",
    "    fig = plt.figure(figsize=(7, 5))\n",
    "    fig.patch.set_visible(False)\n",
    "    sns.barplot(data=lgbm_fi_stacked[lgbm_fi_stacked['product']==product], y='features', x='importance', \n",
    "                    color='#2aa2cc', edgecolor='black', ci=None, order=temp_order)\n",
    "    plt.xlabel('Relative Importance [%]', fontsize=12)\n",
    "    plt.xticks(ticks=np.arange(0, 6, 1), labels=np.arange(0, 6, 1), fontsize=11)\n",
    "    plt.ylabel(None)\n",
    "    plt.yticks(ticks=np.arange(0, 20), labels=temp_order.map(feature_map), fontsize=10)\n",
    "    plt.title(f'Feature Importance - {product}', fontsize=14)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get predictions, residuals, and feature contributions\n",
    "Here we will examine predictions, residuals, and feature contributions as assessed through shap values for the LightGBM model trained with temporal and weather features over a whole year. We will also examine predictions and residuals for the naive baseline model for comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/data_final.csv')\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df.set_index('date', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate train and test df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df[df.year<2018]\n",
    "df_test = df[df.year>=2018]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define feature set\n",
    "weather_features = ['turnover_lag_7', 'turnover_lag_365', 'month_sin', 'month_cos', 'day_of_week', 'school_holiday', 'public_holiday',\n",
    "                                    'temp_mean', 'humidity_mean', 'rain_1h_mean', 'snow_1h_mean',\n",
    "                                                    'day_frosty', 'day_thunder', 'day_clear','day_hazy', 'day_summer',\n",
    "                                                    'temp_mean_dev', 'humidity_mean_dev', 'pressure_mean_dev', 'rain_1h_mean_dev', 'snow_1h_mean_dev',\n",
    "                                                    'temp_mean_change', 'pressure_mean_change', 'humidity_mean_change',\n",
    "                                                    'temp_mean_lead_1', 'rain_1h_mean_lead_1', 'snow_1h_mean_lead_1']\n",
    "\n",
    "# define hyperparameters\n",
    "params_optimal = {\n",
    "    'boosting_type': 'dart',\n",
    "    'n_estimators': 200,\n",
    "    'learning_rate': 0.1\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get predictions, residuals, and shap values for a whole year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_preds = meteo.get_preds_and_shaps(df_train, grouping_vars=['branch', 'product'], target='turnover', features=weather_features,\n",
    "                              lgbm_kwargs=params_optimal, splits=52, compute_baseline=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show and example predictions dataframe for a single time series for inspection\n",
    "model_preds['predictions'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### visualize predictions and residuals for individual time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over all time series and plot predictions and residuals\n",
    "for i in range(len(model_preds['group'])):\n",
    "    data_temp = model_preds['predictions'][i]\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 4))\n",
    "    fig.suptitle(model_preds['group'][i])\n",
    "    sns.lineplot(data=data_temp, x=data_temp.index, y='y_true', color='grey', \n",
    "                        label='observed', ax=ax1)\n",
    "    sns.lineplot(data=data_temp, x=data_temp.index, y='y_pred_baseline', color='#d6633a',\n",
    "                        label='predicted by Baseline', ax=ax1)\n",
    "    sns.lineplot(data=data_temp, x=data_temp.index, y='y_pred_lgbm', color='#1b6883', \n",
    "                        label='predicted by LightGBM', ax=ax1)\n",
    "    ax1.set_ylabel('Turnover [€]', fontsize=12)\n",
    "    ax1.set_ylim(0, data_temp['y_true'].max()+100)\n",
    "    ax1.set_xlabel(None)\n",
    "    ax1.legend(bbox_to_anchor=(1.01, 0.4), loc='upper left', frameon=False, fontsize=10)\n",
    "\n",
    "    sns.scatterplot(data=data_temp, x=data_temp.index, y='residual_baseline', \n",
    "                    color='#d6633a', edgecolor='black', label='Baseline', ax=ax2)\n",
    "    sns.scatterplot(data=data_temp, x=data_temp.index, y='residual_lgbm', \n",
    "                    color='#1b6883', edgecolor='black', label='LightGBM', ax=ax2)\n",
    "    ax2.set_ylabel('Model error [€]', fontsize=12)\n",
    "    ax2.set_ylim((data_temp[['residual_lgbm', 'residual_baseline']].min().min())-100, \n",
    "                    (data_temp[['residual_lgbm', 'residual_baseline']].max().max())+100)\n",
    "    ax2.set_xlabel(None)\n",
    "    ax2.legend(bbox_to_anchor=(1.01, 0.4), loc='upper left', frameon=False, fontsize=10)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sum up residuals to get an estimate of the yearly total error in €\n",
    "We will sum up all individual errors (i.e. residuals) across a whole year to get the yearly summed error per branch/product combination. We will then sum up these errors over all branch/product combinations to get an estimate of the yearly total error in €. We will perform these calculations separately for positive residuals (i.e. errors due to overestimating sales) and negative residuals (i.e. errors due to underestimating sales)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summed_error = pd.DataFrame({'group': [], 'pos_baseline': [], 'neg_baseline': [], 'pos_lgbm': [], 'neg_lgbm': []})\n",
    "for i in range(len(model_preds['group'])):\n",
    "    data_temp = model_preds['predictions'][i]\n",
    "    sum_pos_baseline = data_temp[data_temp['residual_baseline']>=0]['residual_baseline'].sum() \n",
    "    sum_neg_baseline = data_temp[data_temp['residual_baseline']<0]['residual_baseline'].sum() \n",
    "    sum_pos_lgbm = data_temp[data_temp['residual_lgbm']>=0]['residual_lgbm'].sum() \n",
    "    sum_neg_lgbm = data_temp[data_temp['residual_lgbm']<0]['residual_lgbm'].sum() \n",
    "\n",
    "    summed_error.loc[i, 'group'] = model_preds['group'][i]\n",
    "    summed_error.loc[i, 'pos_baseline'] = sum_pos_baseline\n",
    "    summed_error.loc[i, 'neg_baseline'] = np.abs(sum_neg_baseline)\n",
    "    summed_error.loc[i, 'pos_lgbm'] = sum_pos_lgbm\n",
    "    summed_error.loc[i, 'neg_lgbm'] = np.abs(sum_neg_lgbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum over all branch/product combinations\n",
    "summed_error.loc[15, 'group'] = 'sum'\n",
    "summed_error.loc[15, summed_error.columns[1:]] = [np.sum(summed_error[x]) for x in summed_error.columns[1:]]\n",
    "summed_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stack data to prepare for visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group summed errors according to error type and model\n",
    "summed_error_grouped = summed_error.set_index('group').stack().reset_index()\n",
    "summed_error_grouped.columns=['group', 'error_type', 'error']\n",
    "summed_error_grouped['model'] = [x.split('_')[1] for x in summed_error_grouped['error_type']]\n",
    "summed_error_grouped['error_type'] = [x.split('_')[0] for x in summed_error_grouped['error_type']]\n",
    "summed_error_grouped['error_type'].replace('pos', 'overestimation', inplace=True)\n",
    "summed_error_grouped['error_type'].replace('neg', 'underestimation', inplace=True)\n",
    "summed_error_grouped.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot total yearly summed error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(4,3.5))\n",
    "sns.barplot(data=summed_error_grouped[(summed_error_grouped['group']=='sum')].groupby('model').sum().reset_index(),\n",
    "                x='model', y='error', color='#2aa2cc', edgecolor='black', errwidth=0)\n",
    "plt.ylabel('Yearly summed error [€]', fontsize=12)\n",
    "plt.yticks(ticks=np.arange(0, 400001, 100000), fontsize=11)\n",
    "plt.xlabel(None)\n",
    "plt.xticks(ticks=np.arange(0, 2), labels=['Baseline', 'LightGBM weather'], rotation=45, ha='right',  fontsize=12)\n",
    "plt.title('Financial loss', fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot yearly summed error due to both over- and underestimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# color-code according to errors due to over- and underestimation\n",
    "fig = plt.figure(figsize=(4,3.5))\n",
    "sns.barplot(data=summed_error_grouped[(summed_error_grouped['group']=='sum')].groupby('model').sum().reset_index(),\n",
    "                x='model', y='error', color='#1b6883', edgecolor='black', errwidth=0, label='Overestimation')\n",
    "plt.ylabel('Yearly summed error [€]', fontsize=12)\n",
    "sns.barplot(data=summed_error_grouped[(summed_error_grouped['group']=='sum') & (summed_error_grouped['error_type']=='underestimation')],\n",
    "                x='model', y='error', color='#d6633a', edgecolor='black', errwidth=0, label='Underestimation')\n",
    "plt.ylabel('Yearly summed error [€]', fontsize=12)\n",
    "plt.yticks(ticks=np.arange(0, 400001, 100000), fontsize=11)\n",
    "plt.xlabel(None)\n",
    "plt.xticks(ticks=np.arange(0, 2), labels=['Baseline', 'LightGBM weather'], rotation=45, ha='right', fontsize=12)\n",
    "plt.title('Financial loss', fontsize=14)\n",
    "plt.legend(bbox_to_anchor=(1.05, 0.2), loc='upper left', frameon=False, fontsize=11)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot shap values from cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract example shap values for brief inspection\n",
    "model_preds['shap_lgbm'][0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make shap plot for an individual time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(model_preds['group'])):\n",
    "    # extract shap values for individual time series and stack according to features\n",
    "    shap_temp = model_preds['shap_lgbm'][i][weather_features].stack().reset_index()\n",
    "    # drop unused columns created by resetting index and rename\n",
    "    shap_temp.drop(columns=['level_0'], inplace=True)\n",
    "    shap_temp.columns = ['features', 'shap']\n",
    "\n",
    "    # extract feature values for individual time series and min-max scale; fill NaN w/ 1 before stacking, otherwise such rows will be dropped!\n",
    "    scaler = MinMaxScaler()\n",
    "    features_scaled = pd.DataFrame(scaler.fit_transform(model_preds['features_lgbm'][i][weather_features].fillna(1)), columns=weather_features)\n",
    "    # stack features and append last column to temporary shap df\n",
    "    shap_temp['X'] = features_scaled.stack().reset_index().iloc[:, -1]\n",
    "\n",
    "    # get columns ranked by mean absolute shap values and extract sorted column names\n",
    "    shap_order = model_preds['shap_lgbm'][i][weather_features].abs().mean().sort_values(ascending=False).index\n",
    "    # re-order temporary shap df accordingly\n",
    "    shap_temp['features'] = shap_temp['features'].astype('category')\n",
    "    shap_temp['features'].cat.reorder_categories(shap_order, inplace=True)\n",
    "\n",
    "    # plot shap values per branch/product combination\n",
    "    fig, ax = plt.subplots(figsize=(7, 5))\n",
    "    sns.scatterplot(data=shap_temp, y='features', x='shap', hue='X', palette='RdBu_r', edgecolor=None, alpha=0.5, ax=ax)\n",
    "\n",
    "    sm = plt.cm.ScalarMappable(cmap=\"RdBu_r\")\n",
    "    sm.set_array([])\n",
    "    # Remove the legend and add a colorbar\n",
    "    ax.get_legend().remove()\n",
    "    ax.figure.colorbar(sm)\n",
    "\n",
    "    ax.set_xlim(shap_temp['shap'].min()-10, shap_temp['shap'].max()+10)\n",
    "    ax.set_xlabel('Shap value', fontsize=12)\n",
    "    ax.set_ylabel(None)\n",
    "    ax.set_yticks(ticks=np.arange(0, 27))\n",
    "    ax.set_yticklabels(labels=shap_order.map(feature_map), fontsize=10)\n",
    "\n",
    "    plt.title(model_preds['group'][i])\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions on test set\n",
    "Here, we will generate predictions for the test set for defined time window (restricted to 7 days). We will use the first January week in 2019 for illustration. Note that LightGBM reaches stable performance even if it was only trained with data up to 2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = meteo.LGBM_predict(df_train, df_test, grouping_vars=['branch', 'product'], target='turnover', features=weather_features, \n",
    "                    lgbm_kwargs=params_optimal, start_date='2019-01-01', end_date='2019-01-07', compute_shap=True, plot=True, show_baseline=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.8 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5088fa60f7d34fb5f2ba3ff772c32280f8a6f8f3ea142d94c52ee17185bba4b2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
