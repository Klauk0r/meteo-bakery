{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meteo Bakery - Presentation Figures\n",
    "In this notebook, we will generate figures for presentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from itertools import product\n",
    "import os\n",
    "\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "import shap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/data_combined.csv')\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df.set_index('date', inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transform periodic month feature using sine and cosine functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['month_sin'] = df.month.apply(lambda x: np.sin(np.array(x) * np.pi /6))\n",
    "df['month_cos'] = df.month.apply(lambda x: np.cos(np.array(x) * np.pi /6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### select only years up to 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.year<2020]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate lag features\n",
    "Will we use sales with a lag of 7 and 365 days, since these days showed peaks in partical autocorrelation plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function for generating lagged features\n",
    "def get_lag_features(df, grouping_vars, feature, lags):\n",
    "    \"\"\"Takes in a stacked time series dataframe and generates lag features for defined lags and returns dataframe with lags as\n",
    "    additional columns.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Stacked time series dataframe\n",
    "        grouping_vars (list): A list of grouping variables. Currently accepts only a list of two variables.\n",
    "        feature (str): Name of the feature, for which lags should be generated.\n",
    "        lags (list): A list of lags to generate lag features\n",
    "\n",
    "    Returns:\n",
    "        df (pd.DataFrae): A Dataframe containing the lag features as additional columns.\n",
    "    \"\"\"\n",
    "    # initialize empty dataframe\n",
    "    df_lag = pd.DataFrame({})\n",
    "    \n",
    "    for i, group in enumerate(product(df[grouping_vars[0]].unique(), df[grouping_vars[1]].unique())):\n",
    "        # subselect time series and generate lag features\n",
    "        ts = df[(df[grouping_vars[0]]==group[0]) & (df[grouping_vars[1]]==group[1])].copy()\n",
    "        # map feature to dictionary\n",
    "        target_map = ts[feature].to_dict()\n",
    "        # iterate over every lag, map feature according to lag and append to dataframe\n",
    "        for lag in lags:\n",
    "            ts[f'{feature}_lag_{lag}'] = (ts.index - pd.Timedelta(f'{lag} days')).map(target_map)\n",
    "            \n",
    "        df_lag = pd.concat([df_lag, ts], axis=0)\n",
    "    \n",
    "    return df_lag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function for generating lead features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function for generating lead features\n",
    "def get_lead_features(df, grouping_vars, feature, leads):\n",
    "    \"\"\"Takes in a stacked time series dataframe and generates lead features for defined leads and returns dataframe with leads as\n",
    "    additional columns.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Stacked time series dataframe\n",
    "        grouping_vars (list): A list of grouping variables. Currently accepts only a list of two variables.\n",
    "        feature (str): Name of the feature, for which leads should be generated.\n",
    "        leads (list): A list of leads to generate lead features\n",
    "\n",
    "    Returns:\n",
    "        df (pd.DataFrae): A Dataframe containing the lead features as additional columns.\n",
    "    \"\"\"\n",
    "    # initialize empty dataframe\n",
    "    df_lead = pd.DataFrame({})\n",
    "    \n",
    "    for i, group in enumerate(product(df[grouping_vars[0]].unique(), df[grouping_vars[1]].unique())):\n",
    "        # subselect time series and generate lead features\n",
    "        ts = df[(df[grouping_vars[0]]==group[0]) & (df[grouping_vars[1]]==group[1])].copy()\n",
    "        # map feature to dictionary\n",
    "        target_map = ts[feature].to_dict()\n",
    "        # iterate over every lead, map feature according to lead and append to dataframe\n",
    "        for lead in leads:\n",
    "            ts[f'{feature}_lead_{lead}'] = (ts.index + pd.Timedelta(f'{lead} days')).map(target_map)\n",
    "            \n",
    "        df_lead = pd.concat([df_lead, ts], axis=0)\n",
    "    \n",
    "    return df_lead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate lag features for turnover (lag 7, lag 365)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lag = get_lag_features(df, ['branch', 'product'], 'turnover', [7, 365])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lag[(df_lag.index >= pd.to_datetime('2012-12-20')) & (df_lag['branch']=='Metro') & (df_lag['product']=='Brown Bread')][['turnover', 'turnover_lag_7', 'turnover_lag_365']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate lead features for weather (lead 1; temperature, rain, humidity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lag = get_lead_features(df_lag, ['branch', 'product'], 'temp_mean', [1])\n",
    "df_lag = get_lead_features(df_lag, ['branch', 'product'], 'rain_1h_mean', [1])\n",
    "df_lag = get_lead_features(df_lag, ['branch', 'product'], 'snow_1h_mean', [1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### replace missing values\n",
    "Previous analyes showed that a couple of days are missing from the sales data. For the branch located at the Metro and Train Station, there is a total of 4 missing days. By contrast, 69 days are missing for Center branch in the years 2012-2019. They frequently fall on a public holiday, thus indicating that this branch probably had closed on these days.\n",
    "We will first replace NaNs at Center branch by 1 if occuring on public holiday. Remaining NaNs will be replaced with turnover of corresponding day of preceding weak, otherwise, a forward fill will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lag.groupby(['branch', 'product'])['turnover', 'month'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_repl = df_lag.copy()\n",
    "\n",
    "# replace NaN at Center branch by 0 is occuring on public holiday\n",
    "df_repl.loc[(df_repl['branch']=='Center') & (df_repl['public_holiday']==True), 'turnover'] = df_repl.loc[(df_repl['branch']=='Center') & (df_repl['public_holiday']==True), 'turnover'].fillna(1)\n",
    "\n",
    "# fill NaN with sales from previous day of week\n",
    "df_repl['turnover'] = df_repl['turnover'].fillna(df_repl['turnover_lag_7'])\n",
    "\n",
    "# fill remaining NaN using forward fill\n",
    "#df_repl['turnover'].ffill(inplace=True, axis='rows')\n",
    "for i, group in enumerate(product(df_repl['branch'].unique(), df_repl['product'].unique())):\n",
    "        df_repl[(df_repl['branch']==group[0]) & (df_repl['product']==group[1])].ffill(inplace=True, axis='rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if replacement worked properly for an example date with a missing\n",
    "df_repl[(df_repl.index == pd.to_datetime('2012-02-22')) & (df_repl['branch']=='Train_Station')][['turnover', 'turnover_lag_7']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_repl.groupby(['branch', 'product'])[['turnover', 'turnover_lag_7', 'month']].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate train and test df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_repl[df_repl.year<2018]\n",
    "df_test = df_repl[df_repl.year>=2018]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Seasonal baseline\n",
    "We will first define a utility function to perform cross-validation on naive seasonal baseline.\n",
    "\n",
    "#### use sales 14 days ago if prediction contains missing from closed Center branch on public holidays\n",
    "It is possible that a prediction for Naive Seasonal baseline includes values == 1 as prediction, which reflected NaN from public holidays in the preceding 7 day-interval used for prediction in case of Center branch.\n",
    "\n",
    "In such as case, replace any NaN (currently encoded as 1) by the turnover day from exactly 7 days ago. Thus, in case a holiday is contained in the preceding week before prediction, any such holiday is replaced by sales data from the day of the preceding week. Thus, not the sales 7 days before before are used as a prediction when falling on a holiday, but instead the sales 14 days ago.\n",
    "\n",
    "Similarly, it is possible that observations in the validation set contain NaNs. We will extract index positions for such events and then delete them from both validation set and predicted values before computing MAPE score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function for naive seasonal baseline corrected for holiday effects\n",
    "def crossval_naive(df_train, grouping_vars, target, splits=52, test_size=7, gap=0):\n",
    "    \"\"\"Cross-Validation for Naive Seasonal Baseline:\n",
    "    Takes in a training dataset of stacked time series and performs TimeSeriesSplit Cross-Validation for Naive Seasonal \n",
    "    baseline model for each of those time series. Returns a dataframe of cross-validation results containing mean MAPE scores \n",
    "    and corresponding standard deviations from cross-validation of each individual time series.\n",
    "\n",
    "    Args:\n",
    "        df_train (pd.DataFrame): A training dataframe containing stacked time series data\n",
    "        grouping_vars (list): A list of grouping variables, according to which training data is stacked. Currently accepts only a list of two variables.\n",
    "        target (str): Prediction target\n",
    "        splits (int, optional): Number of splits for Cross-Validation. Defaults to 52 (1 fold / week).\n",
    "        test_size (int, optional): Size of validation set (i.e. forecasting horizon). Defaults to 7 days.\n",
    "        gap (int, optional): Time gap between end of training and start of validation set. Defaults to 0.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A dataframe with cross-validation results, containing mean MAPE scores and respective standard deviations for each time series\n",
    "    \"\"\"\n",
    "    # initialize dataframe for evaluation scores\n",
    "    cval = pd.DataFrame({'group': [], 'MAPE_mean': [], 'MAPE_std': []})\n",
    "\n",
    "    # iterate over all individual series and perform cross-validation\n",
    "    for i, group in enumerate(product(df_train[grouping_vars[0]].unique(), df_train[grouping_vars[1]].unique())):\n",
    "\n",
    "        # subselect time series\n",
    "        ts = df_train[(df_train[grouping_vars[0]]==group[0]) & (df_train[grouping_vars[1]]==group[1])].copy()\n",
    "        \n",
    "        # perform cross validation\n",
    "        tss = TimeSeriesSplit(n_splits=splits, test_size=test_size, gap=gap)\n",
    "        # initialize scores list for append MAPE scores from individual folds and start cross-validation\n",
    "        scores = []\n",
    "        for train_i, val_i in tss.split(ts):\n",
    "\n",
    "            y_train = ts.iloc[train_i][target]\n",
    "            y_val = ts.iloc[val_i][target]\n",
    "            \n",
    "            # correct for holiday effects in predicted values based on training set if necessary\n",
    "            # if 1 (representing missing values on a holiday) is in prediced y-values, replace by sales 14 days ago\n",
    "            if 1 in y_train[-7:].unique():\n",
    "                idx_train = [i for i in range(len(y_train[-7:].tolist())) if y_train[-7:].tolist()[i]==1]\n",
    "                idx_train = [i-7 for i in idx_train]\n",
    "                idx_train_lag = [i-7 for i in idx_train]\n",
    "                y_train_repl = y_train.copy()\n",
    "                y_train_repl.iloc[idx_train] = [x for x in y_train.iloc[idx_train_lag]]\n",
    "                y_pred = y_train_repl[-7:]\n",
    "            else:\n",
    "                y_pred = y_train[-7:]\n",
    "\n",
    "            # correct for holiday effects in validation set if necessary\n",
    "            # if 1 (representing missing values on a holiday) is in validation set, drop elements at corresponding index position in both y_val and y_pred\n",
    "            if 1 in y_val.unique():\n",
    "                idx_val = [i for i in range(len(y_val.tolist())) if y_val.tolist()[i]==1]\n",
    "                y_val = y_val.drop(y_val.index[idx_val])\n",
    "                y_pred = y_pred.drop(y_pred.index[idx_val])\n",
    "\n",
    "            mape = mean_absolute_percentage_error(y_val, y_pred)\n",
    "            scores.append(mape)\n",
    "        \n",
    "        # append mean MAPE scores and standart deviations overall all cross-validation folds per time series to dataframe\n",
    "        cval.loc[i, 'group'] = f'{group[0]} | {group[1]}'\n",
    "        cval.loc[i, 'MAPE_mean'] = np.mean(scores)\n",
    "        cval.loc[i, 'MAPE_std'] = np.std(scores)\n",
    "    # calculate mean scores over all time series\n",
    "    cval.loc[i+1, 'group'] = 'mean'\n",
    "    cval.loc[i+1, 'MAPE_mean'] = cval['MAPE_mean'].mean()\n",
    "    cval.loc[i+1, 'MAPE_std'] = cval['MAPE_std'].mean()\n",
    "\n",
    "    return cval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive = crossval_naive(df_train, grouping_vars=['branch', 'product'], target='turnover')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM\n",
    "We will iterate over every time series and evaluate LightGBM performance with only temporal and additional weather features using TimeSeriesSplit Cross-Validation.\n",
    "\n",
    "First, we will define a utility function performing Cross-Validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function for LightGBM\n",
    "def crossval_lgbm(df_train, grouping_vars, target, features, lgbm_kwargs=None, splits=52, test_size=7, gap=0):\n",
    "    \"\"\"Cross-Validation for LightGBM model:\n",
    "    Takes in a training dataset of stacked time series and performs TimeSeriesSplit Cross-Validation for LightGBM model \n",
    "    for each of those time series. Returns a dataframe of cross-validation results containing mean MAPE scores \n",
    "    and corresponding standard deviations from cross-validation of each individual time series.\n",
    "\n",
    "    Args:\n",
    "        df_train (pd.DataFrame): A training dataframe containing stacked time series data\n",
    "        grouping_vars (list): A list of grouping variables, according to which training data is stacked. Currently accepts only a list of two variables.\n",
    "        target (str): Prediction target\n",
    "        features (list): List of feature names to be used for training the model\n",
    "        lgbm_kwargs (dict, optional): Dictionary of LGBM hyperparameters. Defaults to None. If None, model is trained using default hyperparameters.\n",
    "        splits (int, optional): Number of splits for Cross-Validation. Defaults to 52 (1 fold / week).\n",
    "        test_size (int, optional): Size of validation set (i.e. forecasting horizon). Defaults to 7 days.\n",
    "        gap (int, optional): Time gap between end of training and start of validation set. Defaults to 0.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A dataframe with cross-validation results, containing mean MAPE scores and respective standard deviations for each time series\n",
    "    \"\"\"\n",
    "    # initialize dataframe for evaluation scores\n",
    "    cval = pd.DataFrame({'group': [], 'MAPE_mean': [], 'MAPE_std': []})\n",
    "\n",
    "    # iterate over all individual series and perform cross-validation\n",
    "    from itertools import product\n",
    "    for i, group in enumerate(product(df_train[grouping_vars[0]].unique(), df_train[grouping_vars[1]].unique())):\n",
    "\n",
    "        # subselect time series\n",
    "        ts = df_train[(df_train[grouping_vars[0]]==group[0]) & (df_train[grouping_vars[1]]==group[1])].copy()\n",
    "\n",
    "        # perform cross validation\n",
    "        tss = TimeSeriesSplit(n_splits=splits, test_size=test_size, gap=gap)\n",
    "        # initialize scores list for append MAPE scores from individual folds and start cross-validation\n",
    "        scores = []\n",
    "        for train_i, val_i in tss.split(ts):\n",
    "\n",
    "            train = ts.iloc[train_i]\n",
    "            val = ts.iloc[val_i]\n",
    "\n",
    "            # generate target and feature vectors\n",
    "            X_train = train[features]\n",
    "            X_val = val[features]\n",
    "            y_train = train[target]\n",
    "            y_val = val[target]\n",
    "\n",
    "            # initialize model\n",
    "            if lgbm_kwargs==None:\n",
    "                lgbm = LGBMRegressor(objective='regression', random_state=42)\n",
    "            else:\n",
    "                lgbm = LGBMRegressor(objective='regression', random_state=42, **lgbm_kwargs)\n",
    "            # train model\n",
    "            lgbm.fit(X_train, y_train)\n",
    "            # predict\n",
    "            y_pred= pd.Series(lgbm.predict(X_val))\n",
    "\n",
    "            # correct for holiday effects in validation set if necessary\n",
    "            # if 1 (representing missing values on a holiday) is in validation set, drop elements at corresponding index position in both y_val and y_pred\n",
    "            if 1 in y_val.unique():\n",
    "                idx_val = [i for i in range(len(y_val.tolist())) if y_val.tolist()[i]==1]\n",
    "                y_val = y_val.drop(y_val.index[idx_val])\n",
    "                y_pred = y_pred.drop(y_pred.index[idx_val])\n",
    "\n",
    "            mape = mean_absolute_percentage_error(y_val, y_pred)\n",
    "            scores.append(mape)\n",
    "        \n",
    "        # append mean MAPE scores and standart deviations overall all cross-validation folds per time series to dataframe\n",
    "        cval.loc[i, 'group'] = f'{group[0]} | {group[1]}'\n",
    "        cval.loc[i, 'MAPE_mean'] = np.mean(scores)\n",
    "        cval.loc[i, 'MAPE_std'] = np.std(scores)\n",
    "    # calculate mean scores over all time series\n",
    "    cval.loc[i+1, 'group'] = 'mean'\n",
    "    cval.loc[i+1, 'MAPE_mean'] = cval['MAPE_mean'].mean()\n",
    "    cval.loc[i+1, 'MAPE_std'] = cval['MAPE_std'].mean()\n",
    "    \n",
    "    return cval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyperparameters based on previous gridsearch results\n",
    "params_optimal = {\n",
    "    'boosting_type': 'dart',\n",
    "    'n_estimators': 100,\n",
    "    'learning_rate': 0.1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define features sets\n",
    "time_features = ['turnover_lag_7', 'turnover_lag_365', 'month_sin', 'month_cos', 'day_of_week', 'school_holiday', 'public_holiday']\n",
    "\n",
    "weather_features = ['turnover_lag_7', 'turnover_lag_365', 'month_sin', 'month_cos', 'day_of_week', 'school_holiday', 'public_holiday',\n",
    "                                    'temp_mean', 'humidity_mean', 'rain_1h_mean', 'snow_1h_mean',\n",
    "                                                    'day_frosty', 'day_thunder', 'day_clear','day_hazy', 'day_summer',\n",
    "                                                    'temp_mean_dev', 'humidity_mean_dev', 'pressure_mean_dev', 'rain_1h_mean_dev', 'snow_1h_mean_dev',\n",
    "                                                    'temp_mean_change', 'pressure_mean_change', 'humidity_mean_change',\n",
    "                                                    'temp_mean_lead_1', 'rain_1h_mean_lead_1', 'snow_1h_mean_lead_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_time = crossval_lgbm(df_train, grouping_vars=['branch', 'product'], target='turnover', features=time_features,\n",
    "                            lgbm_kwargs=params_optimal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_weather = crossval_lgbm(df_train, grouping_vars=['branch', 'product'], target='turnover', features=weather_features,\n",
    "                            lgbm_kwargs=params_optimal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_weather"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### merge cross-validation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_merged = pd.concat([naive, lgbm_time[['MAPE_mean', 'MAPE_std']], lgbm_weather[['MAPE_mean', 'MAPE_std']]], axis=1)\n",
    "scores_merged.columns = ['group', 'MAPE_mean_naive', 'MAPE_std_naive', 'MAPE_mean_lgbm_time', 'MAPE_std_lgbm_time',\n",
    "                            'MAPE_mean_lgbm_weather', 'MAPE_std_lgbm_weather']\n",
    "\n",
    "scores_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save cross-validation results\n",
    "scores_merged.to_csv('../models/lgbm_optimized/CV_baseline_lgbm.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_grouped = pd.DataFrame(scores_merged[['group', 'MAPE_mean_naive', 'MAPE_mean_lgbm_time', 'MAPE_mean_lgbm_weather']].set_index('group').stack().reset_index().iloc[:-3, :])\n",
    "scores_grouped.columns = ['group', 'model', 'MAPE_mean']\n",
    "scores_grouped['MAPE_std'] = pd.DataFrame(scores_merged[['group',  'MAPE_std_naive', 'MAPE_std_lgbm_time', 'MAPE_std_lgbm_weather']].set_index('group').stack().reset_index().iloc[:-3, :])[0]\n",
    "scores_grouped['model'] = [x.split('_')[-1] for x in scores_grouped['model']]\n",
    "scores_grouped[['MAPE_mean', 'MAPE_std']] = scores_grouped[['MAPE_mean', 'MAPE_std']] *100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_grouped.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract branch and product information as separate columns\n",
    "scores_grouped['branch'] = [x.split(' | ')[0] for x in scores_grouped['group']]\n",
    "scores_grouped['product'] = [x.split(' | ')[1] for x in scores_grouped['group']]\n",
    "scores_grouped.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot mean MAPE and standard deviation from cross-validation over all groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(3,3))\n",
    "#fig.patch.set_visible(False)\n",
    "sns.barplot(data=scores_grouped, x='model', y='MAPE_mean', palette=['#96ed89', '#4192d9', '#2c1dff'], edgecolor='black', errwidth=0)\n",
    "plt.ylabel('Average prediction error [%]', fontsize=12)\n",
    "plt.yticks(np.arange(0, 26, 5), fontsize=11)\n",
    "plt.xlabel(None)\n",
    "plt.xticks(ticks=np.arange(0, 3), labels=['Baseline', 'LightGBM time', 'LightGBM weather'], fontsize=12, rotation=45, ha='right');\n",
    "plt.title('Model Comparison', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(3,3))\n",
    "#fig.patch.set_visible(False)\n",
    "sns.barplot(data=scores_grouped, x='model', y='MAPE_std', palette=['#96ed89', '#4192d9', '#2c1dff'], edgecolor='black', errwidth=0)\n",
    "plt.ylabel('Variability of prediction error [%]', fontsize=12)\n",
    "plt.yticks(np.arange(0, 26, 5), fontsize=11)\n",
    "plt.xlabel(None)\n",
    "plt.xticks(ticks=np.arange(0, 3), labels=['Baseline', 'LightGBM time', 'LightGBM weather'], fontsize=12, rotation=45, ha='right');\n",
    "plt.title('Model Comparison', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot scores separaly for each branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(4,3))\n",
    "#fig.patch.set_visible(False)\n",
    "sns.barplot(data=scores_grouped, x='branch', y='MAPE_mean', edgecolor='black', errwidth=0, hue='model', palette=['#96ed89', '#4192d9', '#2c1dff'],\n",
    "                    order=['Metro', 'Train_Station', 'Center'])\n",
    "plt.ylabel('Average prediction error [%]', fontsize=12)\n",
    "plt.yticks(np.arange(0, 26, 5), fontsize=11)\n",
    "plt.xlabel(None)\n",
    "plt.xticks(ticks=np.arange(0, 3), labels=['Metro', 'Train Station', 'Center'], fontsize=12, rotation=45, ha='right');\n",
    "plt.title('Model Comparison', fontsize=14)\n",
    "leg= plt.legend(bbox_to_anchor=(1.01, 0.4), loc='upper left', frameon=False, fontsize=11)\n",
    "leg.get_texts()[0].set_text('Baseline')\n",
    "leg.get_texts()[1].set_text('LightGBM time')\n",
    "leg.get_texts()[2].set_text('LightGBM weather')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(4,3))\n",
    "fig.patch.set_visible(False)\n",
    "sns.barplot(data=scores_grouped, x='branch', y='MAPE_std', edgecolor='black', errwidth=0, hue='model', palette=['#96ed89', '#4192d9', '#2c1dff'],\n",
    "                    order=['Metro', 'Train_Station', 'Center'])\n",
    "plt.ylabel('Variability of prediction error [%]', fontsize=12)\n",
    "plt.yticks(np.arange(0, 26, 5), fontsize=11)\n",
    "plt.xlabel(None)\n",
    "plt.xticks(ticks=np.arange(0, 3), labels=['Metro', 'Train Station', 'Center'], fontsize=12, rotation=45, ha='right');\n",
    "plt.title('Model Comparison', fontsize=14)\n",
    "leg= plt.legend(bbox_to_anchor=(1.01, 0.4), loc='upper left', frameon=False, fontsize=11)\n",
    "leg.get_texts()[0].set_text('Baseline')\n",
    "leg.get_texts()[1].set_text('LightGBM time')\n",
    "leg.get_texts()[2].set_text('LightGBM weather')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calculate feature importance for optimized LGBM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lgbm_feature_importance(df_train, grouping_vars, target, features, lgbm_kwargs=None, filepath=None):\n",
    "    \"\"\"Extracting feature importance from LightGBM model:\n",
    "    Trains separate LightGBM models for individual time series in a stacked time series training dataframe and extracts feature importances.\n",
    "    Returns dataframe containing feature importances for each individual time series. \n",
    "\n",
    "    Args:\n",
    "        df_train (pd.DataFrame): A training dataframe containing stacked time series data\n",
    "        grouping_vars (list): A list of grouping variables, according to which training data is stacked. Currently accepts only a list of two variables.\n",
    "        target (str): Prediction target\n",
    "        features (list): List of feature names to be used for training the model\n",
    "        lgbm_kwargs (dict, optional): Dictionary of LGBM hyperparameters. Defaults to None. If None, model is trained using default hyperparameters.\n",
    "        filepath (str, optional): File path for saving trained model. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: a dataframe containing feature importances for all indivudal time series.\n",
    "    \"\"\"\n",
    "    # initialize empty dataframe with group column and feature columns\n",
    "    fimportance = pd.DataFrame({}, columns=['group']+features)\n",
    "\n",
    "    # iterate over all individual series and fit LightGBM model\n",
    "    for i, group in enumerate(product(df_train[grouping_vars[0]].unique(), df_train[grouping_vars[1]].unique())):\n",
    "\n",
    "        # subselect time series\n",
    "        ts_train = df_train[(df_train[grouping_vars[0]]==group[0]) & (df_train[grouping_vars[1]]==group[1])].copy()\n",
    "\n",
    "\n",
    "        X_train = ts_train[features]\n",
    "        y_train = ts_train[target]\n",
    "\n",
    "        if lgbm_kwargs!=None:\n",
    "            lgbm = LGBMRegressor(objective='regression', random_state=42, importance_type='gain', **lgbm_kwargs)\n",
    "        else:\n",
    "            lgbm = LGBMRegressor(objective='regression', random_state=42, importance_type='gain')\n",
    "        \n",
    "        # train model\n",
    "        lgbm.fit(X_train, y_train)\n",
    "        \n",
    "        # save model if filepath specified\n",
    "        if filepath != None:\n",
    "            lgbm.booster_.save_model(filename=os.path.join(filepath, f'lgbm_{group[0]}_{group[1]}.txt'))\n",
    "\n",
    "        # append feature importances per time series to dataframe\n",
    "        fimportance.loc[i, 'group'] = f'{group[0]} | {group[1]}'\n",
    "        fimportance.loc[i, fimportance.columns[1:]] = lgbm.feature_importances_.tolist()\n",
    "    # calculate mean feature importance averaged over all individual time series\n",
    "    fimportance.loc[i+1, 'group'] = 'mean'\n",
    "    fimportance.loc[i+1, fimportance.columns[1:]] = [fimportance[x].mean() for x in fimportance.columns[1:]]\n",
    "\n",
    "    return fimportance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_fimportance = get_lgbm_feature_importance(df_train, grouping_vars=['branch', 'product'], target='turnover', features=weather_features,\n",
    "                            lgbm_kwargs=params_optimal, filepath=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate feature importance by gain in percent relative to total gain for that time series\n",
    "lgbm_fimportance_rel = lgbm_fimportance.copy()\n",
    "lgbm_fimportance_rel['sum'] = lgbm_fimportance_rel[lgbm_fimportance_rel.columns[1:]].sum(axis=1)\n",
    "\n",
    "lgbm_fimportance_rel[lgbm_fimportance_rel.columns[1:]] = lgbm_fimportance_rel[lgbm_fimportance_rel.columns[1:]].div(lgbm_fimportance_rel['sum'], axis=0) * 100\n",
    "lgbm_fimportance_rel.drop(columns=['sum'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace mean with averaged relative importances per over all branch/product combination\n",
    "lgbm_fimportance_rel.loc[15, lgbm_fimportance_rel.columns[1:]] = [np.mean(lgbm_fimportance_rel[x]) for x in lgbm_fimportance_rel.columns[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_fi_stacked = lgbm_fimportance_rel.set_index('group').stack().reset_index()\n",
    "lgbm_fi_stacked.columns = ['group', 'features', 'importance']\n",
    "lgbm_fi_stacked = lgbm_fi_stacked[lgbm_fi_stacked['group']!='mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract branch and product information as separate columns\n",
    "lgbm_fi_stacked['branch'] = [x.split(' | ')[0] for x in lgbm_fi_stacked['group']]\n",
    "lgbm_fi_stacked['product'] = [x.split(' | ')[1] for x in lgbm_fi_stacked['group']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_order_mean = lgbm_fimportance_rel.set_index('group').sort_values(by='mean', axis=1, ascending=False).columns\n",
    "col_order_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename features during plotting using feature map\n",
    "feature_map = {'turnover_lag_7':'turnover [lag 7]', \n",
    "                'day_of_week':'day of week', \n",
    "                'public_holiday': 'public holiday', \n",
    "                'turnover_lag_365': 'turnover [lag 365]',\n",
    "                'temp_mean': 'temperature [daily mean]', \n",
    "                'snow_1h_mean_dev': 'snowfall [season. dev.]', \n",
    "                'month_cos': 'month [cosine-t.]', \n",
    "                'school_holiday': 'school holiday',\n",
    "                'temp_mean_dev': 'temperature [season. dev.]', \n",
    "                'temp_mean_lead_1': 'temperature [next day]', \n",
    "                'month_sin': 'month [sine-t.]', \n",
    "                'rain_1h_mean_dev': 'rainfall [season. dev.]',\n",
    "                'humidity_mean': 'humidity [daily mean]', \n",
    "                'pressure_mean_dev': 'atm. pressure [season. dev.]', \n",
    "                'humidity_mean_dev': 'humidity [season. dev.]',\n",
    "                'pressure_mean_change': 'atm. pressure [change]', \n",
    "                'temp_mean_change': 'temperature [change]', \n",
    "                'humidity_mean_change': 'humidity [change]',\n",
    "                'rain_1h_mean': 'rainfall [daily mean]', \n",
    "                'rain_1h_mean_lead_1': 'rainfall [next day]', \n",
    "                'day_hazy': 'hazy day', \n",
    "                'day_clear': 'clear day',\n",
    "                'day_frosty': 'frosty day', \n",
    "                'day_summer': 'summer day', \n",
    "                'snow_1h_mean_lead_1': 'snowfall [next day]', \n",
    "                'snow_1h_mean': 'snowfall [daily mean]',\n",
    "                'day_thunder': 'thunder day'\n",
    "                }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot global feature importance averaged over all groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(7, 6))\n",
    "#fig.patch.set_visible(False)\n",
    "sns.barplot(data=lgbm_fi_stacked, y='features', x='importance', color='#2c1dff', edgecolor='black', errwidth=0, order=col_order_mean)\n",
    "plt.xlabel('Relative Importance [%]', fontsize=12)\n",
    "plt.xticks(ticks=np.arange(0, 81, 20), labels=np.arange(0, 81, 20), fontsize=11)\n",
    "plt.ylabel(None)\n",
    "plt.yticks(ticks=np.arange(0, 27), labels=col_order_mean.map(feature_map), fontsize=10)\n",
    "plt.title('Feature Importance', fontsize=14)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot feature importance of weather features only separately for each branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for product in lgbm_fi_stacked['product'].unique():\n",
    "    fimp = lgbm_fimportance_rel.copy()\n",
    "    fimp.drop(['turnover_lag_7', 'turnover_lag_365', 'month_sin', 'month_cos', 'day_of_week', 'school_holiday', 'public_holiday'], \n",
    "                    axis=1, inplace=True)\n",
    "    feature_cols = fimp.columns[1:]\n",
    "    fimp.loc[:14, 'branch'] = [x.split(' | ')[0] for x in fimp.loc[:14, 'group']]\n",
    "    fimp.loc[:14, 'product'] = [x.split(' | ')[1] for x in fimp.loc[:14, 'group']]\n",
    "    fimp = fimp[fimp['product']==product]\n",
    "    fimp.loc[3, 'group'] = 'mean'\n",
    "    fimp.loc[3, feature_cols] = [np.mean(fimp[x]) for x in feature_cols]\n",
    "    temp_order = fimp[fimp.columns[:-2]].set_index('group').sort_values(by='mean', axis=1, ascending=False).columns\n",
    "    \n",
    "    fig = plt.figure(figsize=(4, 5))\n",
    "    fig.patch.set_visible(False)\n",
    "    sns.barplot(data=lgbm_fi_stacked[lgbm_fi_stacked['product']==product], y='features', x='importance', \n",
    "                    color='#2c1dff', edgecolor='blue', errwidth=0, order=temp_order)\n",
    "    plt.xlabel('Relative Importance [%]', fontsize=12)\n",
    "    plt.xticks(ticks=np.arange(0, 6, 1), labels=np.arange(0, 6, 1), fontsize=11)\n",
    "    plt.ylabel(None)\n",
    "    plt.yticks(ticks=np.arange(0, 20), labels=temp_order.map(feature_map), fontsize=10)\n",
    "    plt.title(f'Feature Importance - {product}', fontsize=14)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predict new cases\n",
    "Here, we will generate predictions for the test set. First, we will define a utility function to perform predictions on the test set for defined time window (restricted to 7 days)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function for predicting y-values using LightGBM for specified time window\n",
    "def LGBM_predict(df_train, df_test, grouping_vars, target, features, lgbm_kwargs, start_date, end_date, compute_shap=False, plot=False, show_baseline=False):\n",
    "    \"\"\"Predict target values using LightGBM for defined time window:\n",
    "    Fits a LightGBM model to each individual time series and generates a prediction based on a specified time window in the test dataset.\n",
    "    Can accept any time window length specified by start_date and end_date. However, it is highly recommended to set the prediction time window \n",
    "    to a maximum of 7 days. Also computes shap values and plots prediction results for specified time window if specified.\n",
    "\n",
    "    Args:\n",
    "        df_train (pd.DataFrame): A training dataframe containing stacked time series data\n",
    "        df_test (pd.DataFrame): A test dataframe containing stacked time series data\n",
    "        grouping_vars (list): A list of grouping variables, according to which training data is stacked. Currently accepts only a list of two variables.\n",
    "        target (str): Prediction target\n",
    "        features (list): List of feature names to be used for training the model\n",
    "        lgbm_kwargs (dict, optional): Dictionary of LGBM hyperparameters. Defaults to None. If None, model is trained using default hyperparameters.\n",
    "        start_date (str): Start date of prediction time window\n",
    "        end_date (str): End date of prediction time window\n",
    "        compute_shap (bool, optional): Compute shap values for each prediction. Defaults to False.\n",
    "        plot (bool, optional): Display time series plots for observed and predicted values. Defaults to False.\n",
    "        show_baseline (bool, optional): Include predictions by Seasonal Naive baseline in time series plots. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containg predicted y-values (y_pred), observed y-values (y_true), MAPE scores and shap values associated with\n",
    "        the predictions for each individual time series.\n",
    "    \"\"\"\n",
    "    # initialize dataframe for evaluation scores\n",
    "    preds = {'group': [], 'y_pred': [], 'y_true': [], 'MAPE': [], 'shap': []}\n",
    "\n",
    "    # iterate over all individual series\n",
    "    for i, group in enumerate(product(df_test[grouping_vars[0]].unique(), df_test[grouping_vars[1]].unique())):\n",
    "\n",
    "        # subselect time series for train and test\n",
    "        ts_test = df_test[(df_test[grouping_vars[0]]==group[0]) & (df_test[grouping_vars[1]]==group[1])].copy()\n",
    "        ts_train = df_train[(df_train[grouping_vars[0]]==group[0]) & (df_train[grouping_vars[1]]==group[1])].copy()\n",
    "        \n",
    "        # generate target and feature vectors\n",
    "        X_train = ts_train[features]\n",
    "        X_test = ts_test[features]\n",
    "        y_train = ts_train[target]\n",
    "        y_test = ts_test[target]\n",
    "\n",
    "        # initialize model\n",
    "        if lgbm_kwargs==None:\n",
    "            lgbm = LGBMRegressor(objective='regression', random_state=42)\n",
    "        else:\n",
    "            lgbm = LGBMRegressor(objective='regression', random_state=42, **lgbm_kwargs)\n",
    "        # fit model to train data\n",
    "        lgbm.fit(X_train, y_train)\n",
    "\n",
    "        # extract 7d prediction sample and predict\n",
    "        y_test_sample = y_test[(y_test.index >= pd.to_datetime(start_date)) & (y_test.index <= pd.to_datetime(end_date))]\n",
    "        X_test_sample = X_test[(X_test.index >= pd.to_datetime(start_date)) & (X_test.index <= pd.to_datetime(end_date))]\n",
    "        y_pred = pd.Series(lgbm.predict(X_test_sample))\n",
    "\n",
    "        # also extract prediction from naive baseline to show for comparison if required\n",
    "        y_pred_naive = X_test_sample['turnover_lag_7'].fillna(1)\n",
    "        # correct for holiday effects; if holiday is in prediced y-values, replace by sales 14 days ago\n",
    "        if 1 in y_pred_naive.unique():\n",
    "            idx_naive = [i for i in range(len(y_pred_naive.tolist())) if y_pred_naive.tolist()[i]==1]\n",
    "            idx_naive = [i-7 for i in idx_naive]\n",
    "            idx_naive_lag = [i-7 for i in idx_naive]\n",
    "            y_pred_naive = X_test.loc[(X_test.index <= pd.to_datetime(end_date)), 'turnover_lag_7']\n",
    "            y_pred_naive.iloc[idx_naive] = [x for x in y_train.iloc[idx_naive_lag]]\n",
    "            y_pred_naive = y_pred_naive[-7:]\n",
    "        \n",
    "        # correct for holiday effects in validation set if necessary\n",
    "        # if holiday is in validation set, drop elements at corresponding index position in both y_test and y_pred\n",
    "        if 1 in y_test_sample.unique():\n",
    "            idx_test = [i for i in range(len(y_test_sample.tolist())) if y_test_sample.tolist()[i]==1]\n",
    "            y_test_sample = y_test_sample.drop(y_test_sample.index[idx_test])\n",
    "            y_pred = y_pred.drop(y_pred.index[idx_test])\n",
    "            y_pred_naive = y_pred_naive.drop(y_pred_naive.index[idx_test])\n",
    "\n",
    "        # compute MAPE\n",
    "        mape = mean_absolute_percentage_error(y_test_sample, y_pred)\n",
    "\n",
    "        # append results\n",
    "        preds['group'].append(f'{group[0]} | {group[1]}')\n",
    "        preds['y_pred'].append([y_pred])\n",
    "        preds['y_true'].append([y_test_sample])\n",
    "        preds['MAPE'].append(mape)\n",
    "\n",
    "        # compute shap values\n",
    "        if compute_shap==True:\n",
    "            explainer = shap.Explainer(lgbm)\n",
    "            shap_values = explainer(X_test_sample)\n",
    "            preds['shap'].append(shap_values)\n",
    "        \n",
    "        # plot prediction results over observed values\n",
    "        if plot==True:\n",
    "            sample_data = y_test_sample.reset_index()\n",
    "            sample_data['y_pred_LGBM'] = y_pred.values\n",
    "            sample_data['y_pred_naive'] = y_pred_naive.values\n",
    "            sample_data.columns = ['date', 'observed', 'y_pred_LGBM', 'y_pred_naive']\n",
    "            \n",
    "            plt.figure(figsize=(6, 2))\n",
    "            sns.lineplot(data=sample_data, x='date', y='observed', color='black')\n",
    "            # plot also baseline predictions for reference if required\n",
    "            if show_baseline==True:\n",
    "                sns.lineplot(data=sample_data, x='date', y='y_pred_naive', color='red', marker='o')\n",
    "            sns.lineplot(data=sample_data, x='date', y='y_pred_LGBM', color='blue', marker='o')\n",
    "            plt.ylabel('Turnover [€]', fontsize=12)\n",
    "            plt.yticks(fontsize=12)\n",
    "            plt.ylim(0, np.max(sample_data['observed'])+100)\n",
    "            plt.xlabel(None)\n",
    "            plt.xticks(rotation=45, ha='right', fontsize=11)\n",
    "            if show_baseline==True:\n",
    "                plt.legend(labels=['observed', 'predicted by baseline', 'predicted by LGBM'], bbox_to_anchor=(1.05, 1.0), loc='upper left')\n",
    "            else:\n",
    "                plt.legend(labels=['observed', 'predicted'], bbox_to_anchor=(1.05, 1.0), loc='upper left')\n",
    "            plt.title(f'{group[0]} | {group[1]}\\n{start_date} - {end_date}', fontsize=14)\n",
    "            plt.show()\n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lgbm_preds = LGBM_predict(df_train, df_test, grouping_vars=['branch', 'product'], target='turnover', features=weather_climat_dev_features, \n",
    "#                    lgbm_kwargs=params_optimal, start_date='2019-01-01', end_date='2019-01-07', compute_shap=True, plot=True, show_baseline=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make 1-year prediction\n",
    "We will perform an error analysis based on the residuals generated by the optimized LGBM model. To this end, we will first define a utility function to extract observed values, predicted values, and residuals from cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function for LightGBM\n",
    "def compare_models(df_train, grouping_vars, target, features, lgbm_kwargs=None, splits=52, test_size=7, gap=0):\n",
    "    \"\"\"Compute residuals based on predictions generated by LightGBM across TimeSeriesSplit Cross-Validation folds.\n",
    "    Takes in a training dataset of stacked time series and performs TimeSeriesSplit Cross-Validation for LightGBM model \n",
    "    for each of those time series. Returns a dataframe of observed and predicted target values and corresponding residuals from\n",
    "    subsequent cross-validation folds per individual time series.\n",
    "\n",
    "    Args:\n",
    "        df_train (pd.DataFrame): A training dataframe containing stacked time series data\n",
    "        grouping_vars (list): A list of grouping variables, according to which training data is stacked. Currently accepts only a list of two variables.\n",
    "        target (str): Prediction target\n",
    "        features (list): List of feature names to be used for training the model\n",
    "        lgbm_kwargs (dict, optional): Dictionary of LGBM hyperparameters. Defaults to None. If None, model is trained using default hyperparameters.\n",
    "        splits (int, optional): Number of splits for Cross-Validation. Defaults to 52 (1 fold / week).\n",
    "        test_size (int, optional): Size of validation set (i.e. forecasting horizon). Defaults to 7 days.\n",
    "        gap (int, optional): Time gap between end of training and start of validation set. Defaults to 0.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing datasets of observed and LightGBM-predicted target values and corresponding residuals \n",
    "        from subsequent cross-validation folds per time series.\n",
    "    \"\"\"\n",
    "    # initialize dataframe for evaluation scores\n",
    "    resids = {'group': [], 'combined': []}\n",
    "\n",
    "    # iterate over all individual series and perform cross-validation\n",
    "    from itertools import product\n",
    "    for i, group in enumerate(product(df_train[grouping_vars[0]].unique(), df_train[grouping_vars[1]].unique())):\n",
    "\n",
    "        # subselect time series\n",
    "        ts = df_train[(df_train[grouping_vars[0]]==group[0]) & (df_train[grouping_vars[1]]==group[1])].copy()\n",
    "\n",
    "        # perform cross validation\n",
    "        tss = TimeSeriesSplit(n_splits=splits, test_size=test_size, gap=gap)\n",
    "        # initialize empty dataframe for concatenating results from individual cross-validation folds\n",
    "        combined_local = pd.DataFrame({})\n",
    "        for train_i, val_i in tss.split(ts):\n",
    "\n",
    "            train = ts.iloc[train_i]\n",
    "            val = ts.iloc[val_i]\n",
    "\n",
    "            # generate target and feature vectors\n",
    "            X_train = train[features]\n",
    "            X_val = val[features]\n",
    "            y_train = train[target]\n",
    "            y_val = val[target]\n",
    "\n",
    "            # initialize LGBM model\n",
    "            if lgbm_kwargs==None:\n",
    "                lgbm = LGBMRegressor(objective='regression', random_state=42)\n",
    "            else:\n",
    "                lgbm = LGBMRegressor(objective='regression', random_state=42, **lgbm_kwargs)\n",
    "            # train model\n",
    "            lgbm.fit(X_train, y_train)\n",
    "\n",
    "            y_pred_lgbm= pd.Series(lgbm.predict(X_val))\n",
    "\n",
    "            # make prediction using Naive Seasonal baseline\n",
    "            # correct for holiday effects in predicted values based on training set if necessary\n",
    "            # if 1 (representing missing values on a holiday) is in predicted y-values, replace by sales 14 days ago\n",
    "            if 1 in y_train[-7:].unique():\n",
    "                idx_train = [i for i in range(len(y_train[-7:].tolist())) if y_train[-7:].tolist()[i]==1]\n",
    "                idx_train = [i-7 for i in idx_train]\n",
    "                idx_train_lag = [i-7 for i in idx_train]\n",
    "                y_train_repl = y_train.copy()\n",
    "                y_train_repl.iloc[idx_train] = [x for x in y_train.iloc[idx_train_lag]]\n",
    "                y_pred_naive = y_train_repl[-7:]\n",
    "            else:\n",
    "                y_pred_naive = y_train[-7:]\n",
    "\n",
    "            # correct for holiday effects in validation set if necessary\n",
    "            # if holiday is in validation set, drop elements at corresponding index position in both y_val and y_pred\n",
    "            if 1 in y_val.unique():\n",
    "                idx_val = [i for i in range(len(y_val.tolist())) if y_val.tolist()[i]==1]\n",
    "                y_val = y_val.drop(y_val.index[idx_val])\n",
    "                y_pred_lgbm = y_pred_lgbm.drop(y_pred_lgbm.index[idx_val])\n",
    "                y_pred_naive = y_pred_naive.drop(y_pred_naive.index[idx_val])\n",
    "\n",
    "            combined = pd.DataFrame(y_val.copy())\n",
    "            combined.columns = ['y_true']\n",
    "            combined['y_pred_lgbm'] = y_pred_lgbm.values\n",
    "            combined['y_pred_baseline'] = y_pred_naive.values\n",
    "            # here, we will subtract observed from predicted values, such that positive residuals correspond to over-estimation and negative residuals to underestimation\n",
    "            combined['residual_lgbm'] = combined['y_pred_lgbm'] - combined['y_true']\n",
    "            combined['residual_baseline'] = combined['y_pred_baseline'] - combined['y_true']\n",
    "\n",
    "            combined_local = pd.concat([combined_local, combined], axis=0)\n",
    "\n",
    "        # append scores\n",
    "        resids['group'].append(f'{group[0]} | {group[1]}')\n",
    "        resids['combined'].append(combined_local)\n",
    "    \n",
    "    return resids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_preds = compare_models(df_train, grouping_vars=['branch', 'product'], target='turnover', features=weather_features,\n",
    "                              lgbm_kwargs=params_optimal, splits=52)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_preds['combined'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 4))\n",
    "fig.suptitle(model_preds['group'][0])\n",
    "sns.lineplot(data=model_preds['combined'][0], x=model_preds['combined'][0].index, y='y_true', color='grey', \n",
    "                    label='observed', ax=ax1)\n",
    "sns.lineplot(data=model_preds['combined'][0], x=model_preds['combined'][0].index, y='y_pred_baseline', color='#96ed89',\n",
    "                    label='predicted by Baseline', ax=ax1)\n",
    "sns.lineplot(data=model_preds['combined'][0], x=model_preds['combined'][0].index, y='y_pred_lgbm', color='#2c1dff', \n",
    "                    label='predicted by LightGBM', ax=ax1)\n",
    "ax1.set_ylabel('Turnover [€]', fontsize=12)\n",
    "ax1.set_yticks(ticks=np.arange(0, 401, 100))\n",
    "ax1.set_yticklabels(labels=np.arange(0, 401, 100), fontsize=11)\n",
    "ax1.set_xlabel(None)\n",
    "ax1.legend(bbox_to_anchor=(1.01, 0.4), loc='upper left', frameon=False, fontsize=10)\n",
    "\n",
    "sns.scatterplot(data=model_preds['combined'][0], x=model_preds['combined'][0].index, y='residual_baseline', \n",
    "                color='#96ed89', edgecolor='black', label='Baseline', ax=ax2)\n",
    "sns.scatterplot(data=model_preds['combined'][0], x=model_preds['combined'][0].index, y='residual_lgbm', \n",
    "                color='#2c1dff', edgecolor='black', label='LightGBM', ax=ax2)\n",
    "ax2.set_ylabel('Model error [€]', fontsize=12)\n",
    "ax2.set_yticks(ticks=np.arange(-200, 201, 100))\n",
    "ax2.set_yticklabels(labels=np.arange(-200, 201, 100), fontsize=11)\n",
    "ax2.set_xlabel(None)\n",
    "ax2.legend(bbox_to_anchor=(1.01, 0.4), loc='upper left', frameon=False, fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summed_error = pd.DataFrame({'group': [], 'pos_baseline': [], 'neg_baseline': [], 'pos_lgbm': [], 'neg_lgbm': []})\n",
    "for i in range(len(model_preds['group'])):\n",
    "    data_temp = model_preds['combined'][i]\n",
    "    sum_pos_baseline = data_temp[data_temp['residual_baseline']>=0]['residual_baseline'].sum() \n",
    "    sum_neg_baseline = data_temp[data_temp['residual_baseline']<0]['residual_baseline'].sum() \n",
    "    sum_pos_lgbm = data_temp[data_temp['residual_lgbm']>=0]['residual_lgbm'].sum() \n",
    "    sum_neg_lgbm = data_temp[data_temp['residual_lgbm']<0]['residual_lgbm'].sum() \n",
    "\n",
    "    summed_error.loc[i, 'group'] = model_preds['group'][i]\n",
    "    summed_error.loc[i, 'pos_baseline'] = sum_pos_baseline\n",
    "    summed_error.loc[i, 'neg_baseline'] = sum_neg_baseline\n",
    "    summed_error.loc[i, 'pos_lgbm'] = sum_pos_lgbm\n",
    "    summed_error.loc[i, 'neg_lgbm'] = sum_neg_lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summed_error.loc[15, 'group'] = 'sum'\n",
    "summed_error.loc[15, summed_error.columns[1:]] = [np.sum(summed_error[x]) for x in summed_error.columns[1:]]\n",
    "summed_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summed_error_grouped = summed_error.set_index('group').stack().reset_index()\n",
    "summed_error_grouped.columns=['group', 'error_type', 'error']\n",
    "summed_error_grouped['model'] = [x.split('_')[1] for x in summed_error_grouped['error_type']]\n",
    "summed_error_grouped['error_type'] = [x.split('_')[0] for x in summed_error_grouped['error_type']]\n",
    "summed_error_grouped['error_type'].replace('pos', 'overestimation', inplace=True)\n",
    "summed_error_grouped['error_type'].replace('neg', 'underestimation', inplace=True)\n",
    "summed_error_grouped.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(4,3))\n",
    "#fig.patch.set_visible(False)\n",
    "sns.barplot(data=summed_error_grouped[summed_error_grouped['group']=='sum'], \n",
    "                x='error_type', y='error', edgecolor='black', errwidth=0, hue='model', palette={'baseline':'#34831B', 'lgbm':'#0C2E3A'})\n",
    "plt.ylabel('Yearly summed error [€]', fontsize=12)\n",
    "plt.yticks(ticks=np.arange(-200000, 200001, 50000), fontsize=11)\n",
    "plt.xlabel(None)\n",
    "plt.xticks(ticks=np.arange(0, 2), fontsize=12);\n",
    "plt.title('Financial loss\\ndue to over- or underestimation', fontsize=14)\n",
    "leg= plt.legend(bbox_to_anchor=(1.05, 0.3), loc='upper left', frameon=False, fontsize=11)\n",
    "leg.get_texts()[0].set_text('Baseline')\n",
    "leg.get_texts()[1].set_text('LightGBM')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.8 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5088fa60f7d34fb5f2ba3ff772c32280f8a6f8f3ea142d94c52ee17185bba4b2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
