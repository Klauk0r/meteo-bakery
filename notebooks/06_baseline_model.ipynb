{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meteo Bakery - Baseline Model\n",
    "As our first baseline model, we will use a simple heuristic where we use the product sales from individual days of the previous week as a forecast for sales in the upcoming week. These predictions will be made for:\n",
    "* Total sales\n",
    "* Total sales per branch\n",
    "* Total sales per product\n",
    "* Sales for each product per branch\n",
    "\n",
    "We will use RMSE and MAPE as our evaluation metric. Additionally, we will calculate the RMSE and MAPE for overestimated and underestimated sales. For now, we will perform evaluation based on the whole time series.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/data_combined.csv')\n",
    "\n",
    "# parse 'date' to datetime object\n",
    "df.date = pd.to_datetime(df.date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate unstacked dataframe\n",
    "Currently, our dataframe represents a grouped time series (grouped by branch and product). For modeling, the dataframe will be unstacked (i.e. ungrouped), such that the individual time series are represented as separate columns in the dataframe. The ungrouping will be done for every single level of the grouped dataframe. Specifically, we will generate \n",
    "* a time series for total sales, summing up sales over all branches and products\n",
    "* summed time series for the three different bakery branches with sales summed up across products per branch\n",
    "* summed time series for the five different bakery products with sales summed up across branches per product\n",
    "* individual time series per branch and product\n",
    "\n",
    "In total, we will end up with 24 different time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unstack_time_series(df, index, groups, target):\n",
    "    \n",
    "    # create the individual combinations df\n",
    "    df_groups = df.pivot(index=index, columns=groups, values=target)\n",
    "    df_groups.columns = df_groups.columns.to_flat_index().map('{0[0]} | {0[1]}'.format)\n",
    "\n",
    "    # create df for first group, use agg(pd.Series.sum) instead of .sum to enable skipna, otherwise NaN rows will add up to 0\n",
    "    df_01 = df.groupby([index, groups[0]])[target] \\\n",
    "                        .agg(pd.Series.sum, skipna=False) \\\n",
    "                        .reset_index(drop=False) \\\n",
    "                        .pivot(index=index, columns=groups[0], values=target)\n",
    "\n",
    "    # create df for second group\n",
    "    df_02 = df.groupby([index, groups[1]])[target] \\\n",
    "                        .agg(pd.Series.sum, skipna=False)\\\n",
    "                        .reset_index(drop=False) \\\n",
    "                        .pivot(index=index, columns=groups[1], values=target)\n",
    "\n",
    "    # create the total level df\n",
    "    df_total = df.groupby(index)[target] \\\n",
    "                .agg(pd.Series.sum, skipna=False)\\\n",
    "                .to_frame() \\\n",
    "                .rename(columns={target: 'total'})\n",
    "\n",
    "    # join the DataFrames\n",
    "    df_unstacked = df_total.join(df_01) \\\n",
    "                                .join(df_02) \\\n",
    "                                .join(df_groups)\n",
    "    df_unstacked.index = pd.to_datetime(df_unstacked.index)\n",
    "    return df_unstacked\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unstacked = unstack_time_series(df, 'date', ['branch', 'product'], 'turnover')\n",
    "df_unstacked.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Modeling\n",
    "For baseline modeling, we simply shift the time series by seven days, thereby using the sales from days of the previous week as predictions for the sales on the respective days of the upcoming week. We calculate the residuals by simply subtracting the shifted time series from the actual time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_by_previous_week(df):\n",
    "\n",
    "    # predict values by imputing sales from the day of the preceding week\n",
    "    df_pred = df.shift(7)\n",
    "\n",
    "    # calculate residuals\n",
    "    df_residual = df_pred - df\n",
    "\n",
    "    return df_pred, df_residual\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred, df_residual = predict_by_previous_week(df_unstacked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check missings due to shifting the time series\n",
    "df_residual.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge original and shifted time series \n",
    "\n",
    "We will now merge the original time series with the shifted one to check if shifting was done properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unstacked_full = df_unstacked.merge(df_pred, on='date', how='left')\n",
    "df_unstacked_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unstacked_full.reset_index(inplace=True)\n",
    "df_unstacked_full.loc[[0+1000, 7+1000, 14+1000, 21+1000, 28+1000, 35+1000], ['date', 'Center_x', 'Center_y']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The series seems to be properly shifted by exactly 7 days."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluate performance\n",
    "We calculate the following main metrics for evaluating model performance: 1) RMSE, 2) MAPE\n",
    "Additionally, we calculate RMSE and MAPE both for predictions overestimating and underestimating sales. \n",
    "Finally, we also calculate the overall mean and standard deviation of the actual time series for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_eval_metrics(df_actual, df_residual):\n",
    "    \n",
    "    # iterate over all time series and calculate evaluation scores using list comprehension\n",
    "    mean_actual = [np.mean(df_actual[col]).round(4) for col in df_residual.columns]\n",
    "    std_actual = [np.std(df_actual[col]).round(4) for col in df_residual.columns]\n",
    "    rmse_total = [np.sqrt(np.mean(np.square(df_residual[col]))).round(4) for col in df_residual.columns]\n",
    "    mape_total = [np.mean(np.abs(df_residual[col]) / df_actual[col]).round(4) for col in df_residual.columns]\n",
    "    rmse_over = [np.sqrt(np.mean(np.square(df_residual[df_residual[col]>0][col]))).round(4) for col in df_residual.columns]\n",
    "    rmse_under = [np.sqrt(np.mean(np.square(df_residual[df_residual[col]<=0][col]))).round(4) for col in df_residual.columns]\n",
    "    mape_over = [np.mean(np.abs(df_residual[df_residual[col]>0][col]) / df_unstacked[col]).round(4) for col in df_residual.columns]\n",
    "    mape_under = [np.mean(np.abs(df_residual[df_residual[col]<=0][col]) / df_unstacked[col]).round(4) for col in df_residual.columns]\n",
    "\n",
    "    # combine to dataframe\n",
    "    df_eval = pd.DataFrame({'groups': df_residual.columns, \n",
    "                            'mean_actual': mean_actual, 'std_actual': std_actual,\n",
    "                            'rmse_total': rmse_total, 'mape_total': mape_total,\n",
    "                            'rmse_over': rmse_over, 'rmse_under': rmse_under, 'mape_over': mape_over, 'mape_under': mape_under})\n",
    "    df_eval.set_index('groups', inplace=True, drop=True)\n",
    "    \n",
    "    # return evaluation metrics\n",
    "    return df_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### evaluation scores for predictions in original metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval = calculate_eval_metrics(df_unstacked, df_residual)\n",
    "df_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error analysis for the three different branches\n",
    "We perform an error analysis by plotting residual plots for the three different bakery branches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(10, 4))\n",
    "fig.suptitle('Residual plots', fontsize=16)\n",
    "\n",
    "sns.scatterplot(x=df_pred['Metro'], y=df_residual['Metro'], color='red', ax=ax1)\n",
    "ax1.set_xlabel('predicted sales', fontsize=12)\n",
    "ax1.set_ylabel('error', fontsize=12)\n",
    "ax1.set_title('Metro', fontsize=14)\n",
    "\n",
    "sns.scatterplot(x=df_pred['Center'], y=df_residual['Center'], color='blue', ax=ax2)\n",
    "ax2.set_xlabel('predicted sales', fontsize=12)\n",
    "ax2.set_ylabel('error', fontsize=12)\n",
    "ax2.set_title('Center', fontsize=14)\n",
    "\n",
    "\n",
    "sns.scatterplot(x=df_pred['Train_Station'], y=df_residual['Train_Station'], color='green', ax=ax3)\n",
    "ax3.set_xlabel('predicted sales', fontsize=12)\n",
    "ax3.set_ylabel('error', fontsize=12)\n",
    "ax3.set_title('Train_Station', fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The residuals are not randomly distributed across the range of predicted sales, irrespective of branch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restack time series for further analysis\n",
    "The unstacked time series will be restacked to represent its grouped structure according to branch and product. This stacked time series can then be used to perform additional analyses with the residuals, e.g. their relationship with weather data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_residual.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-stack product sales for branch 'Metro'\n",
    "metro_resid = pd.melt(df_residual, value_vars=df_residual.columns[9:14], var_name='product', value_name='resid_turnover', ignore_index=False)\n",
    "metro_resid['branch'] = 'Metro'\n",
    "metro_resid['product'] = metro_resid['product'].str.split('|').str[1]\n",
    "metro_resid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-stack product sales for branch 'Center'\n",
    "center_resid = pd.melt(df_residual, value_vars=df_residual.columns[14:19], var_name='product', value_name='resid_turnover', ignore_index=False)\n",
    "center_resid['branch'] = 'Center'\n",
    "center_resid['product'] = center_resid['product'].str.split('|').str[1]\n",
    "center_resid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-stack product sales for branch 'Train_Station'\n",
    "train_resid = pd.melt(df_residual, value_vars=df_residual.columns[19:24], var_name='product', value_name='resid_turnover', ignore_index=False)\n",
    "train_resid['branch'] = 'Train_Station'\n",
    "train_resid['product'] = train_resid['product'].str.split('|').str[1]\n",
    "train_resid.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stack all three time series on top of each other\n",
    "resid_all = pd.concat([metro_resid, center_resid, train_resid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resid_all.sort_index(inplace=True)\n",
    "resid_all.reset_index(inplace=True)\n",
    "resid_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the number of dates per group corresponds with the date range w/o gaps calculated at the beginning using pd.date_range\n",
    "resid_all.groupby(['branch', 'product'])['date'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### merge weather statistics into the restacked time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_stats = pd.read_csv('../data/summary_stats.csv')\n",
    "weather_stats.date = pd.to_datetime(weather_stats.date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resid_joined = resid_all.merge(weather_stats, on=['date'], how='left')\n",
    "resid_joined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resid_joined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the number of dates per group still corresponds with the date range w/o gaps calculated at the beginning using pd.date_range\n",
    "resid_joined.groupby(['branch', 'product'])['date'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse the distribution of residuals depending on weahther condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resid_joined.condition_total.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(data = resid_joined[resid_joined.condition_total.isin(['cloudy', 'rainy', 'clear', 'foggy', 'snowy'])], \n",
    "            hue='condition_total', y='resid_turnover', x='branch');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.8 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5088fa60f7d34fb5f2ba3ff772c32280f8a6f8f3ea142d94c52ee17185bba4b2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
